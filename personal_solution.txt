Model type:
I'd use a Bidirectional LSTM connected to a fully connected deep neural network. I think that the choice of the bidirectional LSTM is perfect for the task in order to create the representation of the raw review text. Before passing the vectorial representation of the raw review text to the first layer of the fully connected neural network, I’d concatenate to that vector also the information also the information from the other fields, so that we can consider both the meaning of the review and the technical metadata about it. I think the choice of the bidirectional LSTM can give the model more info about the meaning of the sentence than a simple LSTM, and we can use it since the training is offline and we have access to the full sentence. Other models are of course useless or would not work well , as using only the connected neural network since we would loose all the info about the review. The bidirectional LSTM is a good choice because thanks to the capability of the three gates of the cell (forget, input, output), can virtually remember an information forever. This is because during the backpropagation through time the model learns what is useful and what is not, and with the forget gate can forget one part of the precedent information, or learn only one part of the input with the input gate. Another interesting option would be GRU but since the big number of reviews the LSTM probably will outperform it.


 
 Input:As I said before I want to use both the data from the text and from the metadata.
I'd keep the information from the hotel name, since is a good indicator for the model to understand the value of the hotel, hotel of bad quality will more probably to have more bad reviews. I'd use one-hot encoding.
 I'd like the Hotel-address, in a similar location can be both good and bad hotels, so I don't think is bringing any added value 
Review date: I think can be an useful temporal indicator. I will convert the month into a cyclic representation using sin/cos functions and the year into a numeric value, representing the number of years of distance since today. I will delete the day.
Hotel number of reviews: Probably I'm going to keep it and rescale in [0,1].
 Reviewer nationality: I don't think it's adding any value, and can bring dangerous racial bias.
Reviewer number reviews: keeping it; scale in [0,1].
Review text: to process it I'm going to remove punctuation, stop words, and non ASCII characters, and I'm going to convert all lower cases. And then since I don't know the distribution of the number of words in each review, I will set a max length to the maximum; 400.

The input shape
Hotel name has been converted into one-hot encoded vector, so the input dimension  will depend on the number of unique names (batch size, unique names).
Review date is now a 2 length vector sin_month, year and depend on the batch (batch_size, sin_month, year)
Number of reviews / reviewer n of reviews -> simple normalized number.
Review: normalized text, wit max length of 400 
 Since I'm using a batch all of the input will have the as first dimension the batch size.
Output:  I'll convert the review-type into a numerical version of it where Bad review = 0, and Good review = 1. The last layer of my model will be composed of two neurons, one will predict the review score and the other one the review type. So there are two output. It will need the loss function to take into consideration both of them. The one prediction will be binary 0 or 1, while the other will be a numeric regression. It's highly probable that the model itself will learn the bounds of the regression [2.5, 10], otherwise it will need to do a clipping of the result.
 Loss:  I'd use two different loss and activation functions, on the two neurons of the last layer. For the neuron that is responsible for the binary prediction the activation function to use is the sigmoid. For the neuron of review score is going to be a linear activation . The total loss could be the sum (or weighted sum) of the single losses. For the binary activation we are going to use a binary cross entropy, while for the review score, since is a regression is going to be a MSE.
Model Configuration:  
a) structure: For the review since I have preprocessed the data and did the tokenization, I have two options for the embedding layer, either create the Bow and train over it or use a pre-trained one like BERT. In one case I have more control over the process, the other is more time/power efficient. After the embedding layer I will use a bidirectional LSTM (since it consist in a double LSTM cell that analyzes the meaning of the transformed sentence in both directions, I expect a better result than a single LSTM cell) to create the vectorial representation of the sentence. I will concatenate to this vector also the information coming from the other fields, the metadata. Fields as Hotel name / n reviews / the number of reviews etc…(this will be done with a concatenation “layer”). Then I will input the resulting vector into the dense deep neural network, I will use ReLU as the deep neural network activation function, but tanh/sigmoid inside the LSTM cells. This is because these are way more stable than Relu when coming to RNN.
b) optimization: In this context there are many moving parts, and hyperparameter optimization is compulsory. For example, the batch size. Since I have so many data points, batching is definitely suggested.
n. of epochs, the n of times that the model will “scan” all the entire dataset.
 Dropout: in my case it is very important to set a good value, otherwise overfitting is possible. Using this I can choose to temporarily shut a percentage of neurons.
Type of regularization L1/L2
Embedding dimension
Whether to use or not an optimizer like Adam, that smooths the gradient and optimize the learning rate.
the dimension of the latent space of the bidirectional LSTM.
the number of hidden layers
the number of neurons of the hidden layers
Regarding the model configuration (5a) I forgot to say about the initialization: For the sigmoid/tanh of the LSTM cell I'll use He-inizialization , while for sigmoid/tanh i’ll use Glorot/xavier. About the normalization: I'll do a batch normalization and a L1/L2.
Score:  Since I have a double output I will evaluate the model in two different ways. For the regression I'll use MSE that tells me how far from the actual prediction we are. I could also build a function to evaluate the prediction on a scale 1-to-10 the prediction. For the classification task, since the classes are balanced, I can use both accuracy and F1 score. To better visualize it, also the confusion matrix is very useful. Finally, I must merge the two evaluations (the score [0-10] and the accuracy) and create a single evaluation score.
