{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Embeddings in Keras**\n",
        "* ## Instead of using one hot encoding for each unique word, more meaningful word embeddings can be used\n",
        "* ## E.g., Word2Vec, GloVe (Global Vectors for Word Representation)\n",
        "> * ### You will study them in details in dedicated classes\n",
        "* ## Here we focus on how to learn a word embedding directly in Keras usinf the ```Embedding``` layer\n",
        "> * ### It learns a linear transformation matrix ***E*** that maps the one hot vectors representing the $n$ distinct words onto lower-dimensional feature vectors, the embedding space of dimension $d$\n",
        "> * ### Then, **to save computation, it stores the lookup table of size $n\\times d$ containing the embedding for each unique word** (index from $0$ to $n-1$)\n",
        ">> ### unfortunately, this means the parameters are not the weight in $E$, but directly the embeding values!\n",
        ">> * #### so $n\\times d$ parameters!\n",
        "> * ### **Example**. With $10$ words and embedding dimension $2$, the lookup table can similar to"
      ],
      "metadata": {
        "id": "XfbhxkCzhLbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "+------------+------------+\n",
        "|   index    |  Embedding |\n",
        "+------------+------------+\n",
        "|     0      | [1.2, 3.1] |\n",
        "|     1      | [0.1, 4.2] |\n",
        "|     2      | [1.0, 3.1] |\n",
        "|     3      | [0.3, 2.1] |\n",
        "|     4      | [2.2, 1.4] |\n",
        "|     5      | [0.7, 1.7] |\n",
        "|     6      | [4.1, 2.0] |\n",
        "|     7      | [0.1, 1.4] |\n",
        "|     8      | [0.2, 1.3] |\n",
        "|     9      | [4.3, 2.7] |\n",
        "+------------+------------+"
      ],
      "metadata": {
        "id": "47GgwDf9RvA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">  ### Thus, the word of index $2$, which would correspond to the one-hot vector $v_2 = (0,0,1,0,0,0,0,0,0,0,0)$, is embedded onto the vector $v_2\\cdot\\boldsymbol{E}=(1.0,3.1)$.\n",
        "> ### **The transofmation matrix $\\boldsymbol{E}$, however, is only symbolic, it is not computed nor stored**\n",
        "> ### **Since we learn directly the results of the dot product input one hot vector dot product $E$,  we do not neither need to perform the dot product**\n",
        "> * ### The main difference with standard word embedding strategies is that embedding trained as any other parameter in the model via backpropagation\n",
        "> * ### Accordingly, it is not able to capture general word semantic similarities, but only some word features specific for the current task  "
      ],
      "metadata": {
        "id": "myNHmM0UR7Ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Keras Embedding Layer** ([docs](https://keras.io/api/layers/core_layers/embedding/))"
      ],
      "metadata": {
        "id": "OaFGDiMaUfSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![emb_layer.png](data:image/png;base64,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)\n"
      ],
      "metadata": {
        "id": "9OBOk4hBVuuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Arguments\n",
        "* ### **input_dim**: Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n",
        "* ### **output_dim**: Integer. Dimension of the dense embedding.\n",
        "* ### **input_length**: Integer. Length of sequence of words received as input (temporal steps of the subsequent LSTM)"
      ],
      "metadata": {
        "id": "vU11wOAnVu2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accordingly, **we do not need to transforms words into their one-hot vectors**, we can supply just their intergers and use the *sparse_categorical_crossentropy* loss\n",
        "# Let's repeat the experiment with Gutemberg ebook"
      ],
      "metadata": {
        "id": "Jne1b7F1WjBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Embedding\n",
        "from keras.layers import Dense\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "5AfHHoBOWElf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://www.gutenberg.org/files/1661/1661-0.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Kk6r1Soy3Qu",
        "outputId": "1c454034-77ad-40c6-de38-28d8b5c53c65"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  593k  100  593k    0     0  1406k      0 --:--:-- --:--:-- --:--:-- 1405k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLf_RvAH-LYR",
        "outputId": "e942825f-9226-4bea-fa15-d8e194bfb188"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1661-0.txt  \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '1661-0.txt'\n",
        "text = open(path).read().lower()\n",
        "print('corpus length, number of characters:', len(text))\n",
        "print(f\"text[:100]:{text[:100]}\")\n",
        "max_size=100000\n",
        "# cutting text due to memory size limitations\n",
        "text = text[:max_size]\n",
        "print('selected length, number of characters:', len(text))"
      ],
      "metadata": {
        "id": "KmKWkLlJcPOV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a039c600-5cb9-4b91-b4e2-12fab846ae70"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus length, number of characters: 581425\n",
            "text[:100]:﻿the project gutenberg ebook of the adventures of sherlock holmes,\n",
            "by arthur conan doyle\n",
            "\n",
            "this ebook\n",
            "selected length, number of characters: 100000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def get_words(text):\n",
        "  text = text.replace('--', ' ')\n",
        "  # split into tokens by white space\n",
        "  words = text.split()\n",
        "  # remove punctuation from each token\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  words = [w.translate(table) for w in words]\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  words = [word for word in words if word.isalpha()]\n",
        "  # make lower case\n",
        "  words = [word.lower() for word in words]\n",
        "  return words\n",
        "\n",
        "words = get_words(text)\n",
        "tot_uniq_words =  len(set(words))\n",
        "print(words[:20])\n",
        "print(f\"Total words: {len(words)}\")\n",
        "print(f\"Unique words: {tot_uniq_words}\")"
      ],
      "metadata": {
        "id": "z-u5D_wUcLKu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc73711-7b76-43a5-9c8d-be023150c09d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['project', 'gutenberg', 'ebook', 'of', 'the', 'adventures', 'of', 'sherlock', 'holmes', 'by', 'arthur', 'conan', 'doyle', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of']\n",
            "Total words: 17106\n",
            "Unique words: 3019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a dictionary of unique strings, associated with a unique integer, starting from ```0``` to ```len(unique_words) - 1```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zzgsUXAOpMwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_words = np.unique(words)\n",
        "unique_word_index = dict((c, i) for i, c in enumerate(unique_words))\n",
        "print(f\"len(unique_words):{len(unique_words)}\")\n",
        "print(f\"Unique words[:10]:{unique_words[:10]}\")"
      ],
      "metadata": {
        "id": "F-3qNmN4pg95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f50f921-f7d4-4e2b-e9a5-ec5a09c9c13c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(unique_words):3019\n",
            "Unique words[:10]:['a' 'abandoned' 'abbots' 'abhorrent' 'able' 'about' 'above' 'abruptly'\n",
            " 'absence' 'absolute']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Then we need to create sequences of training data**\n",
        " * ## we take ```NPREV_WORDS``` words and predict the next one\n",
        " * ## **We now use the integer encoding for strings, as required by the embedding layer**\n"
      ],
      "metadata": {
        "id": "KyncOE3nC1iB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NPREV_WORDS = 5\n",
        "prev_words = []\n",
        "next_word = []\n",
        "for i in range(len(words) - NPREV_WORDS):\n",
        "    seq = [unique_word_index.get(tmp) for tmp in words[i:i + NPREV_WORDS]]\n",
        "    if len(seq)!= NPREV_WORDS:\n",
        "      print(len(seq))\n",
        "    else:\n",
        "      prev_words.append(seq)\n",
        "      # the label\n",
        "      next_word.append(unique_word_index.get(words[i + NPREV_WORDS]))\n",
        "print(prev_words[0:3])\n",
        "print(next_word[0:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QZ0RHcawdRl",
        "outputId": "6791007c-3fc9-4ab8-a32f-2d709c8d72e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2043, 1188, 811, 1812, 2666], [1188, 811, 1812, 2666, 45], [811, 1812, 2666, 45, 1812]]\n",
            "[45, 1812, 2360]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_words = len(prev_words)\n",
        "X = np.array(prev_words)\n",
        "Y = np.array(next_word)\n",
        "print(f\"X[0][0]:{X[0][0]}\")\n",
        "print(f\"Y[0][0]:{Y[0]}\")\n",
        "print(f\"print(X.shape):{X.shape}\")"
      ],
      "metadata": {
        "id": "CRuMg9Kwr_2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating the model**\n",
        "* ## To the embedding layer we need to pass the *number of distinct words* $n$ as first argument, the *embedding size* $d$, and the *input sequence length* $N$ (number of temporal steps)\n",
        "* ## The *embedding* layer has an output dimension $N\\times d$, the embedding of all the input words in this temporal window  "
      ],
      "metadata": {
        "id": "SIXjfgpunkLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remind, the NPREV_WORDS we used now represent the\n",
        "#   numeber of time steps considered in the LSTM recurrence\n",
        "#   so automatically by the LSTM implementation,\n",
        "#   the NPREV_WORDS words are fed to the cell one by one\n",
        "emb_size = 50 # size of the embedding space\n",
        "model = Sequential()\n",
        "nunits = 16 # number of hidden units in LSTMcell\n",
        "model.add(Embedding(tot_uniq_words, emb_size, input_length=NPREV_WORDS))\n",
        "model.add(LSTM(units=nunits))\n",
        "# dense layer stacked atop to classify with softmax\n",
        "model.add(Dense(tot_uniq_words, activation='softmax'))\n",
        "print(model.summary())\n",
        "model_file = \"NWP_EMB\"+str(emb_size)+\"model\"+str(nunits)+\".h5\"\n",
        "import os\n",
        "to_train=True\n",
        "if os.path.isfile(model_file):\n",
        "  to_train= False\n",
        "  model.load_weights(model_file)\n",
        "  print(\"file exists, loading\")\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5*10**-4),\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqwMg8jl0EBP",
        "outputId": "592552f6-748e-4e43-c363-48529aaafa20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (None, 5, 50)             150950    \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 16)                4288      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 3019)              51323     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 206561 (806.88 KB)\n",
            "Trainable params: 206561 (806.88 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isfile(model_file):\n",
        "  # training the model\n",
        "\n",
        "  batch_size = 16\n",
        "  epochs = 650\n",
        "  #epochs = 150\n",
        "  history = model.fit(x=X,y=Y,\n",
        "            epochs=epochs,\n",
        "            batch_size = batch_size,\n",
        "            shuffle=True)\n",
        "  # saving the trained weights\n",
        "  model.save_weights(model_file)"
      ],
      "metadata": {
        "id": "VaUJjNlbmlK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1941bb8d-b7b3-4ca8-c893-e9fc1a62ae12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/650\n",
            "1069/1069 [==============================] - 12s 8ms/step - loss: 6.6500 - accuracy: 0.0549\n",
            "Epoch 2/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 6.1533 - accuracy: 0.0559\n",
            "Epoch 3/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 6.0499 - accuracy: 0.0559\n",
            "Epoch 4/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 5.9897 - accuracy: 0.0558\n",
            "Epoch 5/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 5.9426 - accuracy: 0.0560\n",
            "Epoch 6/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 5.9015 - accuracy: 0.0560\n",
            "Epoch 7/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 5.8636 - accuracy: 0.0559\n",
            "Epoch 8/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 5.8263 - accuracy: 0.0565\n",
            "Epoch 9/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 5.7894 - accuracy: 0.0581\n",
            "Epoch 10/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 5.7530 - accuracy: 0.0614\n",
            "Epoch 11/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 5.7164 - accuracy: 0.0662\n",
            "Epoch 12/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 5.6774 - accuracy: 0.0703\n",
            "Epoch 13/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 5.6380 - accuracy: 0.0713\n",
            "Epoch 14/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 5.5990 - accuracy: 0.0754\n",
            "Epoch 15/650\n",
            "1069/1069 [==============================] - 12s 11ms/step - loss: 5.5572 - accuracy: 0.0777\n",
            "Epoch 16/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 5.5173 - accuracy: 0.0827\n",
            "Epoch 17/650\n",
            "1069/1069 [==============================] - 13s 12ms/step - loss: 5.4770 - accuracy: 0.0832\n",
            "Epoch 18/650\n",
            "1069/1069 [==============================] - 14s 13ms/step - loss: 5.4351 - accuracy: 0.0877\n",
            "Epoch 19/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 5.3937 - accuracy: 0.0887\n",
            "Epoch 20/650\n",
            "1069/1069 [==============================] - 11s 11ms/step - loss: 5.3517 - accuracy: 0.0915\n",
            "Epoch 21/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 5.3103 - accuracy: 0.0936\n",
            "Epoch 22/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 5.2704 - accuracy: 0.0951\n",
            "Epoch 23/650\n",
            "1069/1069 [==============================] - 11s 11ms/step - loss: 5.2308 - accuracy: 0.0982\n",
            "Epoch 24/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 5.1920 - accuracy: 0.1006\n",
            "Epoch 25/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 5.1521 - accuracy: 0.1033\n",
            "Epoch 26/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 5.1128 - accuracy: 0.1043\n",
            "Epoch 27/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 5.0758 - accuracy: 0.1097\n",
            "Epoch 28/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 5.0384 - accuracy: 0.1149\n",
            "Epoch 29/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 5.0013 - accuracy: 0.1191\n",
            "Epoch 30/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 4.9645 - accuracy: 0.1232\n",
            "Epoch 31/650\n",
            "1069/1069 [==============================] - 12s 11ms/step - loss: 4.9305 - accuracy: 0.1275\n",
            "Epoch 32/650\n",
            "1069/1069 [==============================] - 13s 12ms/step - loss: 4.8954 - accuracy: 0.1305\n",
            "Epoch 33/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 4.8621 - accuracy: 0.1330\n",
            "Epoch 34/650\n",
            "1069/1069 [==============================] - 12s 11ms/step - loss: 4.8265 - accuracy: 0.1361\n",
            "Epoch 35/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 4.7934 - accuracy: 0.1393\n",
            "Epoch 36/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 4.7602 - accuracy: 0.1427\n",
            "Epoch 37/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 4.7274 - accuracy: 0.1447\n",
            "Epoch 38/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 4.6944 - accuracy: 0.1483\n",
            "Epoch 39/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 4.6624 - accuracy: 0.1506\n",
            "Epoch 40/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 4.6314 - accuracy: 0.1547\n",
            "Epoch 41/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 4.5999 - accuracy: 0.1572\n",
            "Epoch 42/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 4.5674 - accuracy: 0.1610\n",
            "Epoch 43/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 4.5393 - accuracy: 0.1630\n",
            "Epoch 44/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 4.5088 - accuracy: 0.1669\n",
            "Epoch 45/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 4.4782 - accuracy: 0.1700\n",
            "Epoch 46/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 4.4487 - accuracy: 0.1715\n",
            "Epoch 47/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 4.4194 - accuracy: 0.1762\n",
            "Epoch 48/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 4.3914 - accuracy: 0.1782\n",
            "Epoch 49/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 4.3619 - accuracy: 0.1826\n",
            "Epoch 50/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 4.3355 - accuracy: 0.1861\n",
            "Epoch 51/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 4.3062 - accuracy: 0.1895\n",
            "Epoch 52/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 4.2799 - accuracy: 0.1920\n",
            "Epoch 53/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 4.2530 - accuracy: 0.1978\n",
            "Epoch 54/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 4.2251 - accuracy: 0.1999\n",
            "Epoch 55/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 4.1991 - accuracy: 0.2045\n",
            "Epoch 56/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 4.1726 - accuracy: 0.2078\n",
            "Epoch 57/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 4.1460 - accuracy: 0.2097\n",
            "Epoch 58/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 4.1208 - accuracy: 0.2145\n",
            "Epoch 59/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 4.0961 - accuracy: 0.2171\n",
            "Epoch 60/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 4.0720 - accuracy: 0.2228\n",
            "Epoch 61/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 4.0467 - accuracy: 0.2268\n",
            "Epoch 62/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 4.0214 - accuracy: 0.2291\n",
            "Epoch 63/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.9957 - accuracy: 0.2324\n",
            "Epoch 64/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 3.9733 - accuracy: 0.2373\n",
            "Epoch 65/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.9493 - accuracy: 0.2385\n",
            "Epoch 66/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.9241 - accuracy: 0.2456\n",
            "Epoch 67/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.9028 - accuracy: 0.2467\n",
            "Epoch 68/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 3.8782 - accuracy: 0.2504\n",
            "Epoch 69/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.8571 - accuracy: 0.2529\n",
            "Epoch 70/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.8332 - accuracy: 0.2562\n",
            "Epoch 71/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 3.8114 - accuracy: 0.2617\n",
            "Epoch 72/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 3.7924 - accuracy: 0.2616\n",
            "Epoch 73/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.7684 - accuracy: 0.2658\n",
            "Epoch 74/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.7472 - accuracy: 0.2684\n",
            "Epoch 75/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 3.7270 - accuracy: 0.2723\n",
            "Epoch 76/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 3.7072 - accuracy: 0.2747\n",
            "Epoch 77/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.6851 - accuracy: 0.2759\n",
            "Epoch 78/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.6645 - accuracy: 0.2805\n",
            "Epoch 79/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 3.6445 - accuracy: 0.2852\n",
            "Epoch 80/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.6261 - accuracy: 0.2867\n",
            "Epoch 81/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.6093 - accuracy: 0.2880\n",
            "Epoch 82/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.5859 - accuracy: 0.2931\n",
            "Epoch 83/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 3.5683 - accuracy: 0.2942\n",
            "Epoch 84/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.5502 - accuracy: 0.2998\n",
            "Epoch 85/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.5324 - accuracy: 0.3007\n",
            "Epoch 86/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 3.5144 - accuracy: 0.3012\n",
            "Epoch 87/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 3.4956 - accuracy: 0.3047\n",
            "Epoch 88/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.4783 - accuracy: 0.3082\n",
            "Epoch 89/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.4591 - accuracy: 0.3104\n",
            "Epoch 90/650\n",
            "1069/1069 [==============================] - 8s 8ms/step - loss: 3.4421 - accuracy: 0.3113\n",
            "Epoch 91/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.4244 - accuracy: 0.3162\n",
            "Epoch 92/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.4066 - accuracy: 0.3176\n",
            "Epoch 93/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 3.3922 - accuracy: 0.3220\n",
            "Epoch 94/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 3.3762 - accuracy: 0.3247\n",
            "Epoch 95/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 3.3583 - accuracy: 0.3257\n",
            "Epoch 96/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.3416 - accuracy: 0.3292\n",
            "Epoch 97/650\n",
            "1069/1069 [==============================] - 8s 8ms/step - loss: 3.3246 - accuracy: 0.3310\n",
            "Epoch 98/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.3134 - accuracy: 0.3321\n",
            "Epoch 99/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.2931 - accuracy: 0.3368\n",
            "Epoch 100/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 3.2792 - accuracy: 0.3384\n",
            "Epoch 101/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 3.2619 - accuracy: 0.3431\n",
            "Epoch 102/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.2495 - accuracy: 0.3443\n",
            "Epoch 103/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.2341 - accuracy: 0.3467\n",
            "Epoch 104/650\n",
            "1069/1069 [==============================] - 8s 8ms/step - loss: 3.2181 - accuracy: 0.3486\n",
            "Epoch 105/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.2036 - accuracy: 0.3502\n",
            "Epoch 106/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.1910 - accuracy: 0.3542\n",
            "Epoch 107/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 3.1745 - accuracy: 0.3561\n",
            "Epoch 108/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 3.1616 - accuracy: 0.3586\n",
            "Epoch 109/650\n",
            "1069/1069 [==============================] - 11s 11ms/step - loss: 3.1485 - accuracy: 0.3600\n",
            "Epoch 110/650\n",
            "1069/1069 [==============================] - 13s 12ms/step - loss: 3.1337 - accuracy: 0.3647\n",
            "Epoch 111/650\n",
            "1069/1069 [==============================] - 12s 11ms/step - loss: 3.1205 - accuracy: 0.3649\n",
            "Epoch 112/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 3.1058 - accuracy: 0.3725\n",
            "Epoch 113/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 3.0924 - accuracy: 0.3737\n",
            "Epoch 114/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 3.0807 - accuracy: 0.3737\n",
            "Epoch 115/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 3.0681 - accuracy: 0.3768\n",
            "Epoch 116/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 3.0541 - accuracy: 0.3787\n",
            "Epoch 117/650\n",
            "1069/1069 [==============================] - 13s 12ms/step - loss: 3.0390 - accuracy: 0.3820\n",
            "Epoch 118/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 3.0282 - accuracy: 0.3836\n",
            "Epoch 119/650\n",
            "1069/1069 [==============================] - 12s 11ms/step - loss: 3.0174 - accuracy: 0.3848\n",
            "Epoch 120/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 3.0052 - accuracy: 0.3873\n",
            "Epoch 121/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.9920 - accuracy: 0.3897\n",
            "Epoch 122/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.9797 - accuracy: 0.3929\n",
            "Epoch 123/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.9675 - accuracy: 0.3941\n",
            "Epoch 124/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 2.9547 - accuracy: 0.3980\n",
            "Epoch 125/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.9456 - accuracy: 0.3993\n",
            "Epoch 126/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.9350 - accuracy: 0.4018\n",
            "Epoch 127/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.9207 - accuracy: 0.4033\n",
            "Epoch 128/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.9117 - accuracy: 0.4073\n",
            "Epoch 129/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.8969 - accuracy: 0.4093\n",
            "Epoch 130/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.8885 - accuracy: 0.4099\n",
            "Epoch 131/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 2.8771 - accuracy: 0.4124\n",
            "Epoch 132/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.8691 - accuracy: 0.4158\n",
            "Epoch 133/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.8551 - accuracy: 0.4173\n",
            "Epoch 134/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.8423 - accuracy: 0.4196\n",
            "Epoch 135/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 2.8370 - accuracy: 0.4221\n",
            "Epoch 136/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.8241 - accuracy: 0.4248\n",
            "Epoch 137/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.8110 - accuracy: 0.4258\n",
            "Epoch 138/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.8023 - accuracy: 0.4261\n",
            "Epoch 139/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.7942 - accuracy: 0.4293\n",
            "Epoch 140/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.7850 - accuracy: 0.4314\n",
            "Epoch 141/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.7728 - accuracy: 0.4345\n",
            "Epoch 142/650\n",
            "1069/1069 [==============================] - 8s 8ms/step - loss: 2.7646 - accuracy: 0.4338\n",
            "Epoch 143/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.7552 - accuracy: 0.4378\n",
            "Epoch 144/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.7457 - accuracy: 0.4389\n",
            "Epoch 145/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.7326 - accuracy: 0.4418\n",
            "Epoch 146/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 2.7267 - accuracy: 0.4437\n",
            "Epoch 147/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.7191 - accuracy: 0.4461\n",
            "Epoch 148/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.7080 - accuracy: 0.4468\n",
            "Epoch 149/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.6973 - accuracy: 0.4488\n",
            "Epoch 150/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.6904 - accuracy: 0.4505\n",
            "Epoch 151/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.6817 - accuracy: 0.4521\n",
            "Epoch 152/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.6732 - accuracy: 0.4527\n",
            "Epoch 153/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 2.6667 - accuracy: 0.4544\n",
            "Epoch 154/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.6531 - accuracy: 0.4575\n",
            "Epoch 155/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.6458 - accuracy: 0.4600\n",
            "Epoch 156/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.6395 - accuracy: 0.4585\n",
            "Epoch 157/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 2.6310 - accuracy: 0.4583\n",
            "Epoch 158/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.6244 - accuracy: 0.4634\n",
            "Epoch 159/650\n",
            "1069/1069 [==============================] - 13s 12ms/step - loss: 2.6130 - accuracy: 0.4657\n",
            "Epoch 160/650\n",
            "1069/1069 [==============================] - 12s 11ms/step - loss: 2.6071 - accuracy: 0.4651\n",
            "Epoch 161/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 2.5990 - accuracy: 0.4677\n",
            "Epoch 162/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.5876 - accuracy: 0.4700\n",
            "Epoch 163/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.5804 - accuracy: 0.4724\n",
            "Epoch 164/650\n",
            "1069/1069 [==============================] - 12s 11ms/step - loss: 2.5782 - accuracy: 0.4714\n",
            "Epoch 165/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.5680 - accuracy: 0.4733\n",
            "Epoch 166/650\n",
            "1069/1069 [==============================] - 12s 12ms/step - loss: 2.5598 - accuracy: 0.4752\n",
            "Epoch 167/650\n",
            "1069/1069 [==============================] - 12s 11ms/step - loss: 2.5514 - accuracy: 0.4800\n",
            "Epoch 168/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.5460 - accuracy: 0.4763\n",
            "Epoch 169/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 2.5356 - accuracy: 0.4805\n",
            "Epoch 170/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.5281 - accuracy: 0.4824\n",
            "Epoch 171/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.5218 - accuracy: 0.4851\n",
            "Epoch 172/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 2.5154 - accuracy: 0.4841\n",
            "Epoch 173/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.5087 - accuracy: 0.4868\n",
            "Epoch 174/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.4989 - accuracy: 0.4866\n",
            "Epoch 175/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 2.4923 - accuracy: 0.4893\n",
            "Epoch 176/650\n",
            "1069/1069 [==============================] - 12s 11ms/step - loss: 2.4855 - accuracy: 0.4889\n",
            "Epoch 177/650\n",
            "1069/1069 [==============================] - 14s 13ms/step - loss: 2.4798 - accuracy: 0.4919\n",
            "Epoch 178/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 2.4701 - accuracy: 0.4942\n",
            "Epoch 179/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.4621 - accuracy: 0.4959\n",
            "Epoch 180/650\n",
            "1069/1069 [==============================] - 12s 11ms/step - loss: 2.4563 - accuracy: 0.4953\n",
            "Epoch 181/650\n",
            "1069/1069 [==============================] - 12s 11ms/step - loss: 2.4497 - accuracy: 0.4985\n",
            "Epoch 182/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.4446 - accuracy: 0.4991\n",
            "Epoch 183/650\n",
            "1069/1069 [==============================] - 12s 11ms/step - loss: 2.4341 - accuracy: 0.5007\n",
            "Epoch 184/650\n",
            "1069/1069 [==============================] - 12s 11ms/step - loss: 2.4331 - accuracy: 0.5000\n",
            "Epoch 185/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 2.4241 - accuracy: 0.5034\n",
            "Epoch 186/650\n",
            "1069/1069 [==============================] - 11s 11ms/step - loss: 2.4186 - accuracy: 0.5058\n",
            "Epoch 187/650\n",
            "1069/1069 [==============================] - 14s 13ms/step - loss: 2.4118 - accuracy: 0.5061\n",
            "Epoch 188/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 2.4048 - accuracy: 0.5063\n",
            "Epoch 189/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 2.3976 - accuracy: 0.5077\n",
            "Epoch 190/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.3905 - accuracy: 0.5075\n",
            "Epoch 191/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.3857 - accuracy: 0.5104\n",
            "Epoch 192/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.3818 - accuracy: 0.5105\n",
            "Epoch 193/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 2.3734 - accuracy: 0.5113\n",
            "Epoch 194/650\n",
            "1069/1069 [==============================] - 13s 12ms/step - loss: 2.3631 - accuracy: 0.5156\n",
            "Epoch 195/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 2.3594 - accuracy: 0.5167\n",
            "Epoch 196/650\n",
            "1069/1069 [==============================] - 12s 11ms/step - loss: 2.3588 - accuracy: 0.5135\n",
            "Epoch 197/650\n",
            "1069/1069 [==============================] - 14s 13ms/step - loss: 2.3495 - accuracy: 0.5177\n",
            "Epoch 198/650\n",
            "1069/1069 [==============================] - 12s 12ms/step - loss: 2.3421 - accuracy: 0.5188\n",
            "Epoch 199/650\n",
            "1069/1069 [==============================] - 12s 11ms/step - loss: 2.3391 - accuracy: 0.5187\n",
            "Epoch 200/650\n",
            "1069/1069 [==============================] - 12s 12ms/step - loss: 2.3302 - accuracy: 0.5211\n",
            "Epoch 201/650\n",
            "1069/1069 [==============================] - 13s 13ms/step - loss: 2.3243 - accuracy: 0.5197\n",
            "Epoch 202/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 2.3163 - accuracy: 0.5230\n",
            "Epoch 203/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.3146 - accuracy: 0.5247\n",
            "Epoch 204/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 2.3107 - accuracy: 0.5236\n",
            "Epoch 205/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 2.3016 - accuracy: 0.5246\n",
            "Epoch 206/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.3026 - accuracy: 0.5262\n",
            "Epoch 207/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.2931 - accuracy: 0.5277\n",
            "Epoch 208/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.2820 - accuracy: 0.5306\n",
            "Epoch 209/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.2807 - accuracy: 0.5284\n",
            "Epoch 210/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.2760 - accuracy: 0.5318\n",
            "Epoch 211/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.2713 - accuracy: 0.5300\n",
            "Epoch 212/650\n",
            "1069/1069 [==============================] - 12s 11ms/step - loss: 2.2663 - accuracy: 0.5319\n",
            "Epoch 213/650\n",
            "1069/1069 [==============================] - 13s 12ms/step - loss: 2.2589 - accuracy: 0.5320\n",
            "Epoch 214/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.2565 - accuracy: 0.5338\n",
            "Epoch 215/650\n",
            "1069/1069 [==============================] - 13s 12ms/step - loss: 2.2502 - accuracy: 0.5347\n",
            "Epoch 216/650\n",
            "1069/1069 [==============================] - 13s 12ms/step - loss: 2.2409 - accuracy: 0.5369\n",
            "Epoch 217/650\n",
            "1069/1069 [==============================] - 13s 12ms/step - loss: 2.2403 - accuracy: 0.5358\n",
            "Epoch 218/650\n",
            "1069/1069 [==============================] - 13s 12ms/step - loss: 2.2342 - accuracy: 0.5397\n",
            "Epoch 219/650\n",
            "1069/1069 [==============================] - 13s 12ms/step - loss: 2.2290 - accuracy: 0.5396\n",
            "Epoch 220/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.2261 - accuracy: 0.5394\n",
            "Epoch 221/650\n",
            "1069/1069 [==============================] - 13s 12ms/step - loss: 2.2165 - accuracy: 0.5431\n",
            "Epoch 222/650\n",
            "1069/1069 [==============================] - 12s 12ms/step - loss: 2.2114 - accuracy: 0.5424\n",
            "Epoch 223/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.2063 - accuracy: 0.5434\n",
            "Epoch 224/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.2063 - accuracy: 0.5437\n",
            "Epoch 225/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 2.1967 - accuracy: 0.5455\n",
            "Epoch 226/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.1933 - accuracy: 0.5459\n",
            "Epoch 227/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 2.1898 - accuracy: 0.5475\n",
            "Epoch 228/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 2.1851 - accuracy: 0.5458\n",
            "Epoch 229/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.1800 - accuracy: 0.5472\n",
            "Epoch 230/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.1732 - accuracy: 0.5512\n",
            "Epoch 231/650\n",
            "1069/1069 [==============================] - 13s 12ms/step - loss: 2.1676 - accuracy: 0.5503\n",
            "Epoch 232/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.1668 - accuracy: 0.5523\n",
            "Epoch 233/650\n",
            "1069/1069 [==============================] - 12s 11ms/step - loss: 2.1595 - accuracy: 0.5521\n",
            "Epoch 234/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.1551 - accuracy: 0.5535\n",
            "Epoch 235/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.1513 - accuracy: 0.5556\n",
            "Epoch 236/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.1419 - accuracy: 0.5559\n",
            "Epoch 237/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.1410 - accuracy: 0.5560\n",
            "Epoch 238/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.1401 - accuracy: 0.5558\n",
            "Epoch 239/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 2.1344 - accuracy: 0.5576\n",
            "Epoch 240/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.1244 - accuracy: 0.5591\n",
            "Epoch 241/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.1223 - accuracy: 0.5595\n",
            "Epoch 242/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.1212 - accuracy: 0.5615\n",
            "Epoch 243/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.1124 - accuracy: 0.5624\n",
            "Epoch 244/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.1102 - accuracy: 0.5621\n",
            "Epoch 245/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.1042 - accuracy: 0.5651\n",
            "Epoch 246/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.1060 - accuracy: 0.5616\n",
            "Epoch 247/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 2.0965 - accuracy: 0.5639\n",
            "Epoch 248/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.0934 - accuracy: 0.5652\n",
            "Epoch 249/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 2.0891 - accuracy: 0.5659\n",
            "Epoch 250/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.0888 - accuracy: 0.5666\n",
            "Epoch 251/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 2.0806 - accuracy: 0.5686\n",
            "Epoch 252/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.0782 - accuracy: 0.5689\n",
            "Epoch 253/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.0734 - accuracy: 0.5686\n",
            "Epoch 254/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.0685 - accuracy: 0.5708\n",
            "Epoch 255/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.0612 - accuracy: 0.5714\n",
            "Epoch 256/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.0627 - accuracy: 0.5713\n",
            "Epoch 257/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.0587 - accuracy: 0.5704\n",
            "Epoch 258/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 2.0527 - accuracy: 0.5738\n",
            "Epoch 259/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.0471 - accuracy: 0.5744\n",
            "Epoch 260/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.0458 - accuracy: 0.5744\n",
            "Epoch 261/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.0402 - accuracy: 0.5747\n",
            "Epoch 262/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 2.0352 - accuracy: 0.5749\n",
            "Epoch 263/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 2.0322 - accuracy: 0.5752\n",
            "Epoch 264/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.0284 - accuracy: 0.5782\n",
            "Epoch 265/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 2.0231 - accuracy: 0.5768\n",
            "Epoch 266/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 2.0209 - accuracy: 0.5759\n",
            "Epoch 267/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.0139 - accuracy: 0.5820\n",
            "Epoch 268/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.0139 - accuracy: 0.5806\n",
            "Epoch 269/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 2.0144 - accuracy: 0.5776\n",
            "Epoch 270/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 2.0038 - accuracy: 0.5801\n",
            "Epoch 271/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.9965 - accuracy: 0.5817\n",
            "Epoch 272/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.9969 - accuracy: 0.5821\n",
            "Epoch 273/650\n",
            "1069/1069 [==============================] - 13s 12ms/step - loss: 1.9971 - accuracy: 0.5814\n",
            "Epoch 274/650\n",
            "1069/1069 [==============================] - 12s 11ms/step - loss: 1.9908 - accuracy: 0.5831\n",
            "Epoch 275/650\n",
            "1069/1069 [==============================] - 13s 12ms/step - loss: 1.9854 - accuracy: 0.5835\n",
            "Epoch 276/650\n",
            "1069/1069 [==============================] - 12s 11ms/step - loss: 1.9787 - accuracy: 0.5862\n",
            "Epoch 277/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.9826 - accuracy: 0.5840\n",
            "Epoch 278/650\n",
            "1069/1069 [==============================] - 12s 11ms/step - loss: 1.9731 - accuracy: 0.5864\n",
            "Epoch 279/650\n",
            "1069/1069 [==============================] - 13s 13ms/step - loss: 1.9663 - accuracy: 0.5887\n",
            "Epoch 280/650\n",
            "1069/1069 [==============================] - 13s 12ms/step - loss: 1.9677 - accuracy: 0.5897\n",
            "Epoch 281/650\n",
            "1069/1069 [==============================] - 14s 13ms/step - loss: 1.9610 - accuracy: 0.5893\n",
            "Epoch 282/650\n",
            "1069/1069 [==============================] - 12s 11ms/step - loss: 1.9615 - accuracy: 0.5869\n",
            "Epoch 283/650\n",
            "1069/1069 [==============================] - 13s 12ms/step - loss: 1.9537 - accuracy: 0.5899\n",
            "Epoch 284/650\n",
            "1069/1069 [==============================] - 13s 13ms/step - loss: 1.9520 - accuracy: 0.5910\n",
            "Epoch 285/650\n",
            "1069/1069 [==============================] - 14s 14ms/step - loss: 1.9490 - accuracy: 0.5921\n",
            "Epoch 286/650\n",
            "1069/1069 [==============================] - 13s 13ms/step - loss: 1.9475 - accuracy: 0.5933\n",
            "Epoch 287/650\n",
            "1069/1069 [==============================] - 13s 12ms/step - loss: 1.9405 - accuracy: 0.5921\n",
            "Epoch 288/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.9370 - accuracy: 0.5953\n",
            "Epoch 289/650\n",
            "1069/1069 [==============================] - 11s 11ms/step - loss: 1.9364 - accuracy: 0.5928\n",
            "Epoch 290/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 1.9303 - accuracy: 0.5957\n",
            "Epoch 291/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.9279 - accuracy: 0.5944\n",
            "Epoch 292/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.9234 - accuracy: 0.5972\n",
            "Epoch 293/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.9201 - accuracy: 0.5999\n",
            "Epoch 294/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 1.9176 - accuracy: 0.5990\n",
            "Epoch 295/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.9181 - accuracy: 0.5994\n",
            "Epoch 296/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.9123 - accuracy: 0.5978\n",
            "Epoch 297/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.9059 - accuracy: 0.6005\n",
            "Epoch 298/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 1.9072 - accuracy: 0.6000\n",
            "Epoch 299/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.9031 - accuracy: 0.6015\n",
            "Epoch 300/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.8998 - accuracy: 0.6030\n",
            "Epoch 301/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.8953 - accuracy: 0.6016\n",
            "Epoch 302/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 1.8903 - accuracy: 0.6035\n",
            "Epoch 303/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.8874 - accuracy: 0.6033\n",
            "Epoch 304/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.8853 - accuracy: 0.6043\n",
            "Epoch 305/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.8864 - accuracy: 0.6039\n",
            "Epoch 306/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.8787 - accuracy: 0.6062\n",
            "Epoch 307/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.8760 - accuracy: 0.6051\n",
            "Epoch 308/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.8668 - accuracy: 0.6073\n",
            "Epoch 309/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.8734 - accuracy: 0.6066\n",
            "Epoch 310/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.8671 - accuracy: 0.6082\n",
            "Epoch 311/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.8653 - accuracy: 0.6059\n",
            "Epoch 312/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.8593 - accuracy: 0.6094\n",
            "Epoch 313/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.8572 - accuracy: 0.6069\n",
            "Epoch 314/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.8503 - accuracy: 0.6120\n",
            "Epoch 315/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.8534 - accuracy: 0.6082\n",
            "Epoch 316/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.8488 - accuracy: 0.6118\n",
            "Epoch 317/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.8468 - accuracy: 0.6125\n",
            "Epoch 318/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.8442 - accuracy: 0.6105\n",
            "Epoch 319/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.8402 - accuracy: 0.6127\n",
            "Epoch 320/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.8353 - accuracy: 0.6138\n",
            "Epoch 321/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.8312 - accuracy: 0.6139\n",
            "Epoch 322/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.8272 - accuracy: 0.6128\n",
            "Epoch 323/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.8287 - accuracy: 0.6138\n",
            "Epoch 324/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.8234 - accuracy: 0.6162\n",
            "Epoch 325/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 1.8235 - accuracy: 0.6152\n",
            "Epoch 326/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.8178 - accuracy: 0.6169\n",
            "Epoch 327/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.8128 - accuracy: 0.6187\n",
            "Epoch 328/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.8164 - accuracy: 0.6184\n",
            "Epoch 329/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 1.8120 - accuracy: 0.6159\n",
            "Epoch 330/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.8079 - accuracy: 0.6180\n",
            "Epoch 331/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.8066 - accuracy: 0.6189\n",
            "Epoch 332/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.8018 - accuracy: 0.6202\n",
            "Epoch 333/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 1.8002 - accuracy: 0.6215\n",
            "Epoch 334/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.7930 - accuracy: 0.6215\n",
            "Epoch 335/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.7992 - accuracy: 0.6190\n",
            "Epoch 336/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 1.7950 - accuracy: 0.6204\n",
            "Epoch 337/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.7866 - accuracy: 0.6244\n",
            "Epoch 338/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.7834 - accuracy: 0.6202\n",
            "Epoch 339/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.7859 - accuracy: 0.6246\n",
            "Epoch 340/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 1.7770 - accuracy: 0.6226\n",
            "Epoch 341/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.7764 - accuracy: 0.6257\n",
            "Epoch 342/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.7749 - accuracy: 0.6248\n",
            "Epoch 343/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.7753 - accuracy: 0.6245\n",
            "Epoch 344/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 1.7684 - accuracy: 0.6264\n",
            "Epoch 345/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.7688 - accuracy: 0.6260\n",
            "Epoch 346/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.7636 - accuracy: 0.6274\n",
            "Epoch 347/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.7593 - accuracy: 0.6267\n",
            "Epoch 348/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 1.7606 - accuracy: 0.6270\n",
            "Epoch 349/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.7559 - accuracy: 0.6295\n",
            "Epoch 350/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.7548 - accuracy: 0.6294\n",
            "Epoch 351/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.7510 - accuracy: 0.6278\n",
            "Epoch 352/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.7468 - accuracy: 0.6301\n",
            "Epoch 353/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.7471 - accuracy: 0.6314\n",
            "Epoch 354/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.7485 - accuracy: 0.6300\n",
            "Epoch 355/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.7428 - accuracy: 0.6326\n",
            "Epoch 356/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.7365 - accuracy: 0.6318\n",
            "Epoch 357/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.7343 - accuracy: 0.6334\n",
            "Epoch 358/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.7377 - accuracy: 0.6321\n",
            "Epoch 359/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.7385 - accuracy: 0.6332\n",
            "Epoch 360/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.7319 - accuracy: 0.6334\n",
            "Epoch 361/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.7281 - accuracy: 0.6345\n",
            "Epoch 362/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.7284 - accuracy: 0.6348\n",
            "Epoch 363/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.7222 - accuracy: 0.6350\n",
            "Epoch 364/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.7207 - accuracy: 0.6378\n",
            "Epoch 365/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.7161 - accuracy: 0.6368\n",
            "Epoch 366/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.7163 - accuracy: 0.6370\n",
            "Epoch 367/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.7086 - accuracy: 0.6372\n",
            "Epoch 368/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.7158 - accuracy: 0.6352\n",
            "Epoch 369/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.7118 - accuracy: 0.6363\n",
            "Epoch 370/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.7051 - accuracy: 0.6368\n",
            "Epoch 371/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 1.7051 - accuracy: 0.6375\n",
            "Epoch 372/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.7089 - accuracy: 0.6351\n",
            "Epoch 373/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.7013 - accuracy: 0.6374\n",
            "Epoch 374/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.6991 - accuracy: 0.6384\n",
            "Epoch 375/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.6936 - accuracy: 0.6386\n",
            "Epoch 376/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.6874 - accuracy: 0.6400\n",
            "Epoch 377/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.6925 - accuracy: 0.6391\n",
            "Epoch 378/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.6937 - accuracy: 0.6407\n",
            "Epoch 379/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.6851 - accuracy: 0.6435\n",
            "Epoch 380/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.6810 - accuracy: 0.6410\n",
            "Epoch 381/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.6871 - accuracy: 0.6421\n",
            "Epoch 382/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.6854 - accuracy: 0.6427\n",
            "Epoch 383/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.6796 - accuracy: 0.6409\n",
            "Epoch 384/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.6722 - accuracy: 0.6438\n",
            "Epoch 385/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.6723 - accuracy: 0.6460\n",
            "Epoch 386/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.6764 - accuracy: 0.6442\n",
            "Epoch 387/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.6663 - accuracy: 0.6458\n",
            "Epoch 388/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.6668 - accuracy: 0.6430\n",
            "Epoch 389/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.6704 - accuracy: 0.6442\n",
            "Epoch 390/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.6642 - accuracy: 0.6460\n",
            "Epoch 391/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.6574 - accuracy: 0.6467\n",
            "Epoch 392/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.6651 - accuracy: 0.6425\n",
            "Epoch 393/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.6567 - accuracy: 0.6477\n",
            "Epoch 394/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.6526 - accuracy: 0.6496\n",
            "Epoch 395/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.6566 - accuracy: 0.6456\n",
            "Epoch 396/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.6518 - accuracy: 0.6491\n",
            "Epoch 397/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.6405 - accuracy: 0.6488\n",
            "Epoch 398/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.6461 - accuracy: 0.6509\n",
            "Epoch 399/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.6471 - accuracy: 0.6498\n",
            "Epoch 400/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.6447 - accuracy: 0.6505\n",
            "Epoch 401/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.6399 - accuracy: 0.6537\n",
            "Epoch 402/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.6337 - accuracy: 0.6521\n",
            "Epoch 403/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.6375 - accuracy: 0.6520\n",
            "Epoch 404/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.6364 - accuracy: 0.6510\n",
            "Epoch 405/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.6333 - accuracy: 0.6519\n",
            "Epoch 406/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.6309 - accuracy: 0.6511\n",
            "Epoch 407/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.6365 - accuracy: 0.6523\n",
            "Epoch 408/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.6264 - accuracy: 0.6504\n",
            "Epoch 409/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.6269 - accuracy: 0.6525\n",
            "Epoch 410/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.6199 - accuracy: 0.6547\n",
            "Epoch 411/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.6160 - accuracy: 0.6570\n",
            "Epoch 412/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.6234 - accuracy: 0.6546\n",
            "Epoch 413/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.6170 - accuracy: 0.6547\n",
            "Epoch 414/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.6094 - accuracy: 0.6555\n",
            "Epoch 415/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.6087 - accuracy: 0.6565\n",
            "Epoch 416/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.6140 - accuracy: 0.6535\n",
            "Epoch 417/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.6131 - accuracy: 0.6568\n",
            "Epoch 418/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.6072 - accuracy: 0.6555\n",
            "Epoch 419/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.6071 - accuracy: 0.6574\n",
            "Epoch 420/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.6018 - accuracy: 0.6574\n",
            "Epoch 421/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.6031 - accuracy: 0.6561\n",
            "Epoch 422/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5960 - accuracy: 0.6575\n",
            "Epoch 423/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5995 - accuracy: 0.6578\n",
            "Epoch 424/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.5945 - accuracy: 0.6601\n",
            "Epoch 425/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5857 - accuracy: 0.6612\n",
            "Epoch 426/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5911 - accuracy: 0.6604\n",
            "Epoch 427/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.5879 - accuracy: 0.6589\n",
            "Epoch 428/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.5934 - accuracy: 0.6609\n",
            "Epoch 429/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.5848 - accuracy: 0.6607\n",
            "Epoch 430/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5831 - accuracy: 0.6604\n",
            "Epoch 431/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.5801 - accuracy: 0.6623\n",
            "Epoch 432/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.5904 - accuracy: 0.6601\n",
            "Epoch 433/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.5794 - accuracy: 0.6605\n",
            "Epoch 434/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5753 - accuracy: 0.6609\n",
            "Epoch 435/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5767 - accuracy: 0.6619\n",
            "Epoch 436/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.5714 - accuracy: 0.6634\n",
            "Epoch 437/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.5683 - accuracy: 0.6632\n",
            "Epoch 438/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5694 - accuracy: 0.6668\n",
            "Epoch 439/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.5693 - accuracy: 0.6659\n",
            "Epoch 440/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5641 - accuracy: 0.6656\n",
            "Epoch 441/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.5669 - accuracy: 0.6643\n",
            "Epoch 442/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5651 - accuracy: 0.6656\n",
            "Epoch 443/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5568 - accuracy: 0.6669\n",
            "Epoch 444/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.5583 - accuracy: 0.6654\n",
            "Epoch 445/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.5544 - accuracy: 0.6659\n",
            "Epoch 446/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5530 - accuracy: 0.6684\n",
            "Epoch 447/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5544 - accuracy: 0.6679\n",
            "Epoch 448/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5522 - accuracy: 0.6677\n",
            "Epoch 449/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.5463 - accuracy: 0.6677\n",
            "Epoch 450/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5435 - accuracy: 0.6691\n",
            "Epoch 451/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5511 - accuracy: 0.6679\n",
            "Epoch 452/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.5532 - accuracy: 0.6674\n",
            "Epoch 453/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.5441 - accuracy: 0.6664\n",
            "Epoch 454/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.5415 - accuracy: 0.6698\n",
            "Epoch 455/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.5423 - accuracy: 0.6697\n",
            "Epoch 456/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5408 - accuracy: 0.6691\n",
            "Epoch 457/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.5388 - accuracy: 0.6701\n",
            "Epoch 458/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.5322 - accuracy: 0.6719\n",
            "Epoch 459/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.5363 - accuracy: 0.6721\n",
            "Epoch 460/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5337 - accuracy: 0.6686\n",
            "Epoch 461/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 1.5292 - accuracy: 0.6719\n",
            "Epoch 462/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5320 - accuracy: 0.6723\n",
            "Epoch 463/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5227 - accuracy: 0.6736\n",
            "Epoch 464/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.5216 - accuracy: 0.6728\n",
            "Epoch 465/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.5269 - accuracy: 0.6735\n",
            "Epoch 466/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5198 - accuracy: 0.6733\n",
            "Epoch 467/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5197 - accuracy: 0.6740\n",
            "Epoch 468/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.5234 - accuracy: 0.6728\n",
            "Epoch 469/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.5129 - accuracy: 0.6744\n",
            "Epoch 470/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5104 - accuracy: 0.6777\n",
            "Epoch 471/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5146 - accuracy: 0.6753\n",
            "Epoch 472/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.5110 - accuracy: 0.6755\n",
            "Epoch 473/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.5097 - accuracy: 0.6761\n",
            "Epoch 474/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5018 - accuracy: 0.6787\n",
            "Epoch 475/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.5111 - accuracy: 0.6760\n",
            "Epoch 476/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.5059 - accuracy: 0.6770\n",
            "Epoch 477/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.5057 - accuracy: 0.6760\n",
            "Epoch 478/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4996 - accuracy: 0.6779\n",
            "Epoch 479/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.4986 - accuracy: 0.6778\n",
            "Epoch 480/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.5038 - accuracy: 0.6757\n",
            "Epoch 481/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4918 - accuracy: 0.6784\n",
            "Epoch 482/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4917 - accuracy: 0.6785\n",
            "Epoch 483/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4951 - accuracy: 0.6796\n",
            "Epoch 484/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.4892 - accuracy: 0.6804\n",
            "Epoch 485/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4946 - accuracy: 0.6797\n",
            "Epoch 486/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4889 - accuracy: 0.6791\n",
            "Epoch 487/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4848 - accuracy: 0.6823\n",
            "Epoch 488/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.4833 - accuracy: 0.6794\n",
            "Epoch 489/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4854 - accuracy: 0.6788\n",
            "Epoch 490/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4830 - accuracy: 0.6790\n",
            "Epoch 491/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.4804 - accuracy: 0.6825\n",
            "Epoch 492/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.4844 - accuracy: 0.6814\n",
            "Epoch 493/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4772 - accuracy: 0.6831\n",
            "Epoch 494/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4714 - accuracy: 0.6843\n",
            "Epoch 495/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.4735 - accuracy: 0.6815\n",
            "Epoch 496/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.4746 - accuracy: 0.6838\n",
            "Epoch 497/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4766 - accuracy: 0.6834\n",
            "Epoch 498/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4716 - accuracy: 0.6829\n",
            "Epoch 499/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.4713 - accuracy: 0.6811\n",
            "Epoch 500/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.4652 - accuracy: 0.6833\n",
            "Epoch 501/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4708 - accuracy: 0.6839\n",
            "Epoch 502/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4665 - accuracy: 0.6856\n",
            "Epoch 503/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.4609 - accuracy: 0.6879\n",
            "Epoch 504/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.4647 - accuracy: 0.6852\n",
            "Epoch 505/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4579 - accuracy: 0.6877\n",
            "Epoch 506/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4534 - accuracy: 0.6878\n",
            "Epoch 507/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.4563 - accuracy: 0.6869\n",
            "Epoch 508/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.4587 - accuracy: 0.6872\n",
            "Epoch 509/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.4496 - accuracy: 0.6886\n",
            "Epoch 510/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.4509 - accuracy: 0.6879\n",
            "Epoch 511/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4501 - accuracy: 0.6901\n",
            "Epoch 512/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.4549 - accuracy: 0.6857\n",
            "Epoch 513/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.4468 - accuracy: 0.6867\n",
            "Epoch 514/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.4445 - accuracy: 0.6903\n",
            "Epoch 515/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4419 - accuracy: 0.6908\n",
            "Epoch 516/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.4478 - accuracy: 0.6892\n",
            "Epoch 517/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4376 - accuracy: 0.6908\n",
            "Epoch 518/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4435 - accuracy: 0.6899\n",
            "Epoch 519/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4440 - accuracy: 0.6869\n",
            "Epoch 520/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.4412 - accuracy: 0.6894\n",
            "Epoch 521/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4329 - accuracy: 0.6924\n",
            "Epoch 522/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4378 - accuracy: 0.6902\n",
            "Epoch 523/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.4336 - accuracy: 0.6903\n",
            "Epoch 524/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.4299 - accuracy: 0.6938\n",
            "Epoch 525/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4325 - accuracy: 0.6919\n",
            "Epoch 526/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4293 - accuracy: 0.6913\n",
            "Epoch 527/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.4332 - accuracy: 0.6912\n",
            "Epoch 528/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.4242 - accuracy: 0.6926\n",
            "Epoch 529/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4268 - accuracy: 0.6917\n",
            "Epoch 530/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4174 - accuracy: 0.6940\n",
            "Epoch 531/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.4136 - accuracy: 0.6959\n",
            "Epoch 532/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.4256 - accuracy: 0.6930\n",
            "Epoch 533/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4232 - accuracy: 0.6920\n",
            "Epoch 534/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.4185 - accuracy: 0.6936\n",
            "Epoch 535/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.4164 - accuracy: 0.6950\n",
            "Epoch 536/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.4183 - accuracy: 0.6955\n",
            "Epoch 537/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4093 - accuracy: 0.6963\n",
            "Epoch 538/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4132 - accuracy: 0.6949\n",
            "Epoch 539/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.4133 - accuracy: 0.6959\n",
            "Epoch 540/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.4132 - accuracy: 0.6955\n",
            "Epoch 541/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4109 - accuracy: 0.6960\n",
            "Epoch 542/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4102 - accuracy: 0.6972\n",
            "Epoch 543/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4039 - accuracy: 0.6977\n",
            "Epoch 544/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.3999 - accuracy: 0.6994\n",
            "Epoch 545/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4041 - accuracy: 0.6972\n",
            "Epoch 546/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.4084 - accuracy: 0.6981\n",
            "Epoch 547/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.4008 - accuracy: 0.6978\n",
            "Epoch 548/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.3946 - accuracy: 0.6993\n",
            "Epoch 549/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3975 - accuracy: 0.6991\n",
            "Epoch 550/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3999 - accuracy: 0.6982\n",
            "Epoch 551/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3970 - accuracy: 0.6978\n",
            "Epoch 552/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.3941 - accuracy: 0.6995\n",
            "Epoch 553/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3928 - accuracy: 0.7017\n",
            "Epoch 554/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3903 - accuracy: 0.7019\n",
            "Epoch 555/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3864 - accuracy: 0.7010\n",
            "Epoch 556/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.3904 - accuracy: 0.6987\n",
            "Epoch 557/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3932 - accuracy: 0.6999\n",
            "Epoch 558/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3805 - accuracy: 0.7024\n",
            "Epoch 559/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3835 - accuracy: 0.7025\n",
            "Epoch 560/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.3866 - accuracy: 0.7003\n",
            "Epoch 561/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3873 - accuracy: 0.7013\n",
            "Epoch 562/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3818 - accuracy: 0.7015\n",
            "Epoch 563/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3777 - accuracy: 0.7056\n",
            "Epoch 564/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.3792 - accuracy: 0.7026\n",
            "Epoch 565/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3768 - accuracy: 0.7029\n",
            "Epoch 566/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3766 - accuracy: 0.6993\n",
            "Epoch 567/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3773 - accuracy: 0.7010\n",
            "Epoch 568/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.3761 - accuracy: 0.7016\n",
            "Epoch 569/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3836 - accuracy: 0.7016\n",
            "Epoch 570/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3675 - accuracy: 0.7034\n",
            "Epoch 571/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3635 - accuracy: 0.7074\n",
            "Epoch 572/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.3670 - accuracy: 0.7040\n",
            "Epoch 573/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3636 - accuracy: 0.7059\n",
            "Epoch 574/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3635 - accuracy: 0.7084\n",
            "Epoch 575/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3688 - accuracy: 0.7055\n",
            "Epoch 576/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.3695 - accuracy: 0.7067\n",
            "Epoch 577/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3634 - accuracy: 0.7082\n",
            "Epoch 578/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3612 - accuracy: 0.7065\n",
            "Epoch 579/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3598 - accuracy: 0.7065\n",
            "Epoch 580/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.3636 - accuracy: 0.7072\n",
            "Epoch 581/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3557 - accuracy: 0.7070\n",
            "Epoch 582/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3530 - accuracy: 0.7086\n",
            "Epoch 583/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3528 - accuracy: 0.7080\n",
            "Epoch 584/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.3569 - accuracy: 0.7079\n",
            "Epoch 585/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3616 - accuracy: 0.7072\n",
            "Epoch 586/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3570 - accuracy: 0.7086\n",
            "Epoch 587/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3507 - accuracy: 0.7095\n",
            "Epoch 588/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3454 - accuracy: 0.7108\n",
            "Epoch 589/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.3512 - accuracy: 0.7067\n",
            "Epoch 590/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3436 - accuracy: 0.7117\n",
            "Epoch 591/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3476 - accuracy: 0.7106\n",
            "Epoch 592/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.3460 - accuracy: 0.7091\n",
            "Epoch 593/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.3466 - accuracy: 0.7112\n",
            "Epoch 594/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3457 - accuracy: 0.7083\n",
            "Epoch 595/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3426 - accuracy: 0.7104\n",
            "Epoch 596/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.3367 - accuracy: 0.7105\n",
            "Epoch 597/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.3369 - accuracy: 0.7135\n",
            "Epoch 598/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3413 - accuracy: 0.7118\n",
            "Epoch 599/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3362 - accuracy: 0.7113\n",
            "Epoch 600/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3424 - accuracy: 0.7090\n",
            "Epoch 601/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.3352 - accuracy: 0.7111\n",
            "Epoch 602/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3266 - accuracy: 0.7134\n",
            "Epoch 603/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3336 - accuracy: 0.7114\n",
            "Epoch 604/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3298 - accuracy: 0.7140\n",
            "Epoch 605/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.3293 - accuracy: 0.7142\n",
            "Epoch 606/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3341 - accuracy: 0.7137\n",
            "Epoch 607/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3318 - accuracy: 0.7118\n",
            "Epoch 608/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3293 - accuracy: 0.7130\n",
            "Epoch 609/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 1.3233 - accuracy: 0.7157\n",
            "Epoch 610/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3207 - accuracy: 0.7153\n",
            "Epoch 611/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3290 - accuracy: 0.7135\n",
            "Epoch 612/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.3275 - accuracy: 0.7123\n",
            "Epoch 613/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.3167 - accuracy: 0.7151\n",
            "Epoch 614/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3269 - accuracy: 0.7154\n",
            "Epoch 615/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3217 - accuracy: 0.7147\n",
            "Epoch 616/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.3182 - accuracy: 0.7183\n",
            "Epoch 617/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.3174 - accuracy: 0.7167\n",
            "Epoch 618/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3166 - accuracy: 0.7157\n",
            "Epoch 619/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3138 - accuracy: 0.7159\n",
            "Epoch 620/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.3067 - accuracy: 0.7190\n",
            "Epoch 621/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3159 - accuracy: 0.7160\n",
            "Epoch 622/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3165 - accuracy: 0.7151\n",
            "Epoch 623/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3134 - accuracy: 0.7177\n",
            "Epoch 624/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.3098 - accuracy: 0.7173\n",
            "Epoch 625/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3128 - accuracy: 0.7173\n",
            "Epoch 626/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3111 - accuracy: 0.7181\n",
            "Epoch 627/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3072 - accuracy: 0.7190\n",
            "Epoch 628/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.3065 - accuracy: 0.7181\n",
            "Epoch 629/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.2992 - accuracy: 0.7208\n",
            "Epoch 630/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3020 - accuracy: 0.7173\n",
            "Epoch 631/650\n",
            "1069/1069 [==============================] - 11s 10ms/step - loss: 1.3054 - accuracy: 0.7193\n",
            "Epoch 632/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.3062 - accuracy: 0.7169\n",
            "Epoch 633/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.2985 - accuracy: 0.7203\n",
            "Epoch 634/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.2972 - accuracy: 0.7211\n",
            "Epoch 635/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3013 - accuracy: 0.7183\n",
            "Epoch 636/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 1.3054 - accuracy: 0.7188\n",
            "Epoch 637/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.2934 - accuracy: 0.7209\n",
            "Epoch 638/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.2953 - accuracy: 0.7204\n",
            "Epoch 639/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.2968 - accuracy: 0.7197\n",
            "Epoch 640/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.2916 - accuracy: 0.7200\n",
            "Epoch 641/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.2886 - accuracy: 0.7228\n",
            "Epoch 642/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.2975 - accuracy: 0.7205\n",
            "Epoch 643/650\n",
            "1069/1069 [==============================] - 9s 9ms/step - loss: 1.2881 - accuracy: 0.7206\n",
            "Epoch 644/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.2917 - accuracy: 0.7212\n",
            "Epoch 645/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.3032 - accuracy: 0.7172\n",
            "Epoch 646/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.2907 - accuracy: 0.7185\n",
            "Epoch 647/650\n",
            "1069/1069 [==============================] - 9s 8ms/step - loss: 1.2830 - accuracy: 0.7227\n",
            "Epoch 648/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.2852 - accuracy: 0.7235\n",
            "Epoch 649/650\n",
            "1069/1069 [==============================] - 10s 10ms/step - loss: 1.2801 - accuracy: 0.7210\n",
            "Epoch 650/650\n",
            "1069/1069 [==============================] - 10s 9ms/step - loss: 1.2889 - accuracy: 0.7217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With embedding we did not reach a pleateau, more training epochs likely to be necessary\n",
        "* ## The increased number of parameters need more training epochs, and in general specific refinements"
      ],
      "metadata": {
        "id": "BEirImVmIa_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bidirectional RNNs**\n",
        "* ## In Keras we can implement a bidirection RNN using a ```Bidirectional``` layer ([docs](https://keras.io/api/layers/recurrent_layers/bidirectional/))"
      ],
      "metadata": {
        "id": "UfIFurmKMfqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "keras.layers.Bidirectional(\n",
        "\n",
        ">layer,\n",
        "\n",
        ">merge_mode=\"concat\",\n",
        "\n",
        ">weights=None,\n",
        "\n",
        ">backward_layer=None, **kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "t9wptmNUMnmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main arguments\n",
        "> * ### **layer**: keras.layers.RNN instance, such as keras.layers.LSTM or keras.layers.GRU. Should be a sequence-processing layer\n",
        "> * ### **merge_mode**: Mode by which outputs of the forward and backward RNNs will be combined. One of {\"sum\", \"mul\", \"concat\", \"ave\", None}. Defaults to \"concat\".\n",
        "> * ### **backward_layer**: Optional keras.layers.RNN, or keras.layers.Layer instance to be used to handle backwards input processing.\n",
        ">> * #### If backward_layer is not provided, the layer instance passed as the layer argument will be used to generate the backward layer automatically.\n",
        ">> * #### Can be different than ```layer``` (see docs)"
      ],
      "metadata": {
        "id": "WyIy6MxUMoRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We first prepare re-process the data so as to use one-hot encoding instead of embedding layers (like in the previous lecture)"
      ],
      "metadata": {
        "id": "5kWpdzgCG7CH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding,Dense, Bidirectional, Input\n",
        "import tensorflow as tf\n",
        "\n",
        "path = '1661-0.txt'\n",
        "text = open(path).read().lower()\n",
        "max_size=100000\n",
        "# cutting text due to memory size limitations\n",
        "text = text[:max_size]\n",
        "print('selected length, number of characters:', len(text))\n",
        "\n",
        "import string\n",
        "def get_words(text):\n",
        "  text = text.replace('--', ' ')\n",
        "  # split into tokens by white space\n",
        "  words = text.split()\n",
        "  # remove punctuation from each token\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  words = [w.translate(table) for w in words]\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  words = [word for word in words if word.isalpha()]\n",
        "  # make lower case\n",
        "  words = [word.lower() for word in words]\n",
        "  return words\n",
        "\n",
        "words = get_words(text)\n",
        "tot_uniq_words =  len(set(words))\n",
        "print(words[:200])\n",
        "print(f\"Total words: {len(words)}\")\n",
        "print(f\"Unique words: {tot_uniq_words}\")\n",
        "\n",
        "NPREV_WORDS = 5\n",
        "prev_words = []\n",
        "next_word = []\n",
        "for i in range(len(words) - NPREV_WORDS):\n",
        "    seq = words[i:i + NPREV_WORDS]\n",
        "    if len(seq)!= NPREV_WORDS:\n",
        "      print(len(seq))\n",
        "    else:\n",
        "      prev_words.append(seq)\n",
        "      # the label\n",
        "      next_word.append(words[i + NPREV_WORDS])\n",
        "print(prev_words[0:3])\n",
        "print(next_word[0:3])\n",
        "\n",
        "X = np.zeros((len(prev_words), NPREV_WORDS, tot_uniq_words), dtype=bool)\n",
        "Y = np.zeros((len(next_word), tot_uniq_words), dtype=bool)\n",
        "for i, each_words in enumerate(prev_words):\n",
        "    for j, each_word in enumerate(each_words):\n",
        "        X[i, j, unique_word_index[each_word]] = 1 # position for the one-hot\n",
        "    Y[i, unique_word_index[next_word[i]]] = 1\n",
        "\n",
        "print(f\"X[0][0]:{X[0][0]}\")\n",
        "print(f\"Y[0][0]:{Y[0]}\")\n",
        "print(f\"print(X.shape):{X.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trdFfP3eWlA0",
        "outputId": "91ff9c71-b60e-49d7-b198-648acfffa113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected length, number of characters: 100000\n",
            "['project', 'gutenberg', 'ebook', 'of', 'the', 'adventures', 'of', 'sherlock', 'holmes', 'by', 'arthur', 'conan', 'doyle', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'if', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'ebook', 'title', 'the', 'adventures', 'of', 'sherlock', 'holmes', 'author', 'arthur', 'conan', 'doyle', 'release', 'date', 'november', 'ebook', 'most', 'recently', 'updated', 'october', 'language', 'english', 'character', 'set', 'encoding', 'produced', 'by', 'an', 'anonymous', 'project', 'gutenberg', 'volunteer', 'and', 'jose', 'menendez', 'start', 'of', 'the', 'project', 'gutenberg', 'ebook', 'the', 'adventures', 'of', 'sherlock', 'holmes', 'the', 'adventures', 'of', 'sherlock', 'holmes', 'by', 'arthur', 'conan', 'doyle', 'contents', 'i', 'a', 'scandal', 'in', 'bohemia', 'ii', 'the', 'redheaded', 'league', 'iii', 'a', 'case', 'of', 'identity', 'iv', 'the', 'boscombe', 'valley', 'mystery', 'v', 'the', 'five', 'orange', 'pips', 'vi', 'the', 'man', 'with', 'the', 'twisted', 'lip', 'vii', 'the', 'adventure', 'of', 'the', 'blue', 'carbuncle', 'viii', 'the', 'adventure', 'of', 'the', 'speckled', 'band', 'ix', 'the', 'adventure', 'of', 'the', 'thumb']\n",
            "Total words: 17106\n",
            "Unique words: 3019\n",
            "[['project', 'gutenberg', 'ebook', 'of', 'the'], ['gutenberg', 'ebook', 'of', 'the', 'adventures'], ['ebook', 'of', 'the', 'adventures', 'of']]\n",
            "['adventures', 'of', 'sherlock']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating the Bidirectional model**\n",
        "* ## Use a similar number of parameters used for the one directional one\n",
        "* ## Since it has two \"parallel\" RNN models, to keep the same number of parameters, we need to half the hidden state dimension"
      ],
      "metadata": {
        "id": "y3an_wjNXpw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nhunits = 8\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(NPREV_WORDS, tot_uniq_words)))\n",
        "model.add(Bidirectional(LSTM(units=nhunits)))\n",
        "# dense layer stacked atop to classify with softmax\n",
        "model.add(Dense(tot_uniq_words, activation='softmax'))\n",
        "\n",
        "print(model.summary())\n",
        "model_file = \"NWP_LSTM\"+str(nhunits)+\"_BIDIR.h5\"\n",
        "import os\n",
        "to_train=True\n",
        "if os.path.isfile(model_file):\n",
        "  to_train= False\n",
        "  model.load_weights(model_file)\n",
        "  print(\"file exists, loading\")\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5*10**-4),\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqWUXzqbXyVD",
        "outputId": "a719c462-605d-4332-b0e8-1720e5229bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_4 (Bidirecti  (None, 16)                193792    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 3019)              51323     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 245115 (957.48 KB)\n",
            "Trainable params: 245115 (957.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isfile(model_file):\n",
        "  # training the model\n",
        "\n",
        "  batch_size = 16\n",
        "  epochs = 650\n",
        "  #epochs = 150\n",
        "  history = model.fit(x=X,y=Y,\n",
        "            epochs=epochs,\n",
        "            batch_size = batch_size,\n",
        "            shuffle=True)\n",
        "  # saving the trained weights\n",
        "  model.save_weights(model_file)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x32ngNoPZcJs",
        "outputId": "3776194d-0b4f-446c-93c9-659822e4c70b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/650\n",
            "1069/1069 [==============================] - 21s 15ms/step - loss: 6.7222 - accuracy: 0.0555\n",
            "Epoch 2/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 6.1667 - accuracy: 0.0559\n",
            "Epoch 3/650\n",
            "1069/1069 [==============================] - 21s 20ms/step - loss: 6.0539 - accuracy: 0.0558\n",
            "Epoch 4/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.9795 - accuracy: 0.0560\n",
            "Epoch 5/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 5.9187 - accuracy: 0.0562\n",
            "Epoch 6/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 5.8656 - accuracy: 0.0573\n",
            "Epoch 7/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.8213 - accuracy: 0.0575\n",
            "Epoch 8/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.7847 - accuracy: 0.0573\n",
            "Epoch 9/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 5.7546 - accuracy: 0.0576\n",
            "Epoch 10/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 5.7283 - accuracy: 0.0579\n",
            "Epoch 11/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.7056 - accuracy: 0.0577\n",
            "Epoch 12/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 5.6826 - accuracy: 0.0580\n",
            "Epoch 13/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 5.6619 - accuracy: 0.0587\n",
            "Epoch 14/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.6436 - accuracy: 0.0584\n",
            "Epoch 15/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.6215 - accuracy: 0.0591\n",
            "Epoch 16/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 5.5986 - accuracy: 0.0599\n",
            "Epoch 17/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 5.5752 - accuracy: 0.0605\n",
            "Epoch 18/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.5505 - accuracy: 0.0627\n",
            "Epoch 19/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.5270 - accuracy: 0.0632\n",
            "Epoch 20/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 5.5031 - accuracy: 0.0640\n",
            "Epoch 21/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.4804 - accuracy: 0.0642\n",
            "Epoch 22/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.4579 - accuracy: 0.0651\n",
            "Epoch 23/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.4373 - accuracy: 0.0649\n",
            "Epoch 24/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 5.4159 - accuracy: 0.0647\n",
            "Epoch 25/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.3947 - accuracy: 0.0662\n",
            "Epoch 26/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.3722 - accuracy: 0.0682\n",
            "Epoch 27/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 5.3503 - accuracy: 0.0680\n",
            "Epoch 28/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 5.3281 - accuracy: 0.0708\n",
            "Epoch 29/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.3058 - accuracy: 0.0709\n",
            "Epoch 30/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 5.2825 - accuracy: 0.0710\n",
            "Epoch 31/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.2597 - accuracy: 0.0733\n",
            "Epoch 32/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.2348 - accuracy: 0.0750\n",
            "Epoch 33/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.2101 - accuracy: 0.0772\n",
            "Epoch 34/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 5.1836 - accuracy: 0.0784\n",
            "Epoch 35/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.1559 - accuracy: 0.0838\n",
            "Epoch 36/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.1259 - accuracy: 0.0879\n",
            "Epoch 37/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 5.0963 - accuracy: 0.0930\n",
            "Epoch 38/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.0655 - accuracy: 0.0991\n",
            "Epoch 39/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.0347 - accuracy: 0.1038\n",
            "Epoch 40/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 5.0031 - accuracy: 0.1095\n",
            "Epoch 41/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 4.9708 - accuracy: 0.1130\n",
            "Epoch 42/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 4.9385 - accuracy: 0.1176\n",
            "Epoch 43/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 4.9076 - accuracy: 0.1221\n",
            "Epoch 44/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 4.8740 - accuracy: 0.1274\n",
            "Epoch 45/650\n",
            "1069/1069 [==============================] - 17s 15ms/step - loss: 4.8428 - accuracy: 0.1294\n",
            "Epoch 46/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 4.8117 - accuracy: 0.1317\n",
            "Epoch 47/650\n",
            "1069/1069 [==============================] - 22s 21ms/step - loss: 4.7797 - accuracy: 0.1368\n",
            "Epoch 48/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 4.7466 - accuracy: 0.1411\n",
            "Epoch 49/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 4.7159 - accuracy: 0.1414\n",
            "Epoch 50/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 4.6848 - accuracy: 0.1470\n",
            "Epoch 51/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 4.6532 - accuracy: 0.1492\n",
            "Epoch 52/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 4.6217 - accuracy: 0.1531\n",
            "Epoch 53/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 4.5919 - accuracy: 0.1550\n",
            "Epoch 54/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 4.5589 - accuracy: 0.1585\n",
            "Epoch 55/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 4.5292 - accuracy: 0.1619\n",
            "Epoch 56/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 4.4992 - accuracy: 0.1669\n",
            "Epoch 57/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 4.4676 - accuracy: 0.1684\n",
            "Epoch 58/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 4.4371 - accuracy: 0.1743\n",
            "Epoch 59/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 4.4064 - accuracy: 0.1772\n",
            "Epoch 60/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 4.3764 - accuracy: 0.1804\n",
            "Epoch 61/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 4.3479 - accuracy: 0.1826\n",
            "Epoch 62/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 4.3153 - accuracy: 0.1874\n",
            "Epoch 63/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 4.2881 - accuracy: 0.1901\n",
            "Epoch 64/650\n",
            "1069/1069 [==============================] - 22s 21ms/step - loss: 4.2573 - accuracy: 0.1934\n",
            "Epoch 65/650\n",
            "1069/1069 [==============================] - 20s 19ms/step - loss: 4.2285 - accuracy: 0.1960\n",
            "Epoch 66/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 4.1998 - accuracy: 0.1993\n",
            "Epoch 67/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 4.1711 - accuracy: 0.2026\n",
            "Epoch 68/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 4.1419 - accuracy: 0.2054\n",
            "Epoch 69/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 4.1137 - accuracy: 0.2112\n",
            "Epoch 70/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 4.0861 - accuracy: 0.2146\n",
            "Epoch 71/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 4.0576 - accuracy: 0.2182\n",
            "Epoch 72/650\n",
            "1069/1069 [==============================] - 24s 22ms/step - loss: 4.0314 - accuracy: 0.2214\n",
            "Epoch 73/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 4.0041 - accuracy: 0.2257\n",
            "Epoch 74/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 3.9765 - accuracy: 0.2285\n",
            "Epoch 75/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 3.9520 - accuracy: 0.2329\n",
            "Epoch 76/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 3.9243 - accuracy: 0.2360\n",
            "Epoch 77/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 3.8998 - accuracy: 0.2419\n",
            "Epoch 78/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 3.8720 - accuracy: 0.2437\n",
            "Epoch 79/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 3.8493 - accuracy: 0.2475\n",
            "Epoch 80/650\n",
            "1069/1069 [==============================] - 17s 15ms/step - loss: 3.8227 - accuracy: 0.2512\n",
            "Epoch 81/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 3.7983 - accuracy: 0.2545\n",
            "Epoch 82/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 3.7746 - accuracy: 0.2578\n",
            "Epoch 83/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 3.7500 - accuracy: 0.2606\n",
            "Epoch 84/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 3.7265 - accuracy: 0.2665\n",
            "Epoch 85/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 3.7049 - accuracy: 0.2684\n",
            "Epoch 86/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 3.6801 - accuracy: 0.2731\n",
            "Epoch 87/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 3.6591 - accuracy: 0.2749\n",
            "Epoch 88/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 3.6352 - accuracy: 0.2802\n",
            "Epoch 89/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 3.6139 - accuracy: 0.2838\n",
            "Epoch 90/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 3.5916 - accuracy: 0.2891\n",
            "Epoch 91/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 3.5690 - accuracy: 0.2903\n",
            "Epoch 92/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 3.5490 - accuracy: 0.2952\n",
            "Epoch 93/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 3.5269 - accuracy: 0.2985\n",
            "Epoch 94/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 3.5063 - accuracy: 0.3000\n",
            "Epoch 95/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 3.4830 - accuracy: 0.3059\n",
            "Epoch 96/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 3.4638 - accuracy: 0.3068\n",
            "Epoch 97/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 3.4435 - accuracy: 0.3110\n",
            "Epoch 98/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 3.4213 - accuracy: 0.3144\n",
            "Epoch 99/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 3.4026 - accuracy: 0.3167\n",
            "Epoch 100/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 3.3824 - accuracy: 0.3210\n",
            "Epoch 101/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 3.3623 - accuracy: 0.3265\n",
            "Epoch 102/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 3.3418 - accuracy: 0.3269\n",
            "Epoch 103/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 3.3210 - accuracy: 0.3319\n",
            "Epoch 104/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 3.3021 - accuracy: 0.3351\n",
            "Epoch 105/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 3.2839 - accuracy: 0.3371\n",
            "Epoch 106/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 3.2644 - accuracy: 0.3390\n",
            "Epoch 107/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 3.2480 - accuracy: 0.3427\n",
            "Epoch 108/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 3.2276 - accuracy: 0.3464\n",
            "Epoch 109/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 3.2090 - accuracy: 0.3490\n",
            "Epoch 110/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 3.1910 - accuracy: 0.3538\n",
            "Epoch 111/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 3.1737 - accuracy: 0.3565\n",
            "Epoch 112/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 3.1547 - accuracy: 0.3575\n",
            "Epoch 113/650\n",
            "1069/1069 [==============================] - 17s 15ms/step - loss: 3.1370 - accuracy: 0.3623\n",
            "Epoch 114/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 3.1199 - accuracy: 0.3659\n",
            "Epoch 115/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 3.1017 - accuracy: 0.3678\n",
            "Epoch 116/650\n",
            "1069/1069 [==============================] - 17s 15ms/step - loss: 3.0855 - accuracy: 0.3698\n",
            "Epoch 117/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 3.0682 - accuracy: 0.3750\n",
            "Epoch 118/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 3.0491 - accuracy: 0.3783\n",
            "Epoch 119/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 3.0365 - accuracy: 0.3780\n",
            "Epoch 120/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 3.0175 - accuracy: 0.3825\n",
            "Epoch 121/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 2.9998 - accuracy: 0.3849\n",
            "Epoch 122/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.9845 - accuracy: 0.3881\n",
            "Epoch 123/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.9687 - accuracy: 0.3906\n",
            "Epoch 124/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.9511 - accuracy: 0.3947\n",
            "Epoch 125/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 2.9350 - accuracy: 0.3955\n",
            "Epoch 126/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.9190 - accuracy: 0.3961\n",
            "Epoch 127/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.9043 - accuracy: 0.4030\n",
            "Epoch 128/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.8864 - accuracy: 0.4066\n",
            "Epoch 129/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 2.8742 - accuracy: 0.4080\n",
            "Epoch 130/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.8574 - accuracy: 0.4118\n",
            "Epoch 131/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.8426 - accuracy: 0.4130\n",
            "Epoch 132/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 2.8281 - accuracy: 0.4154\n",
            "Epoch 133/650\n",
            "1069/1069 [==============================] - 17s 15ms/step - loss: 2.8118 - accuracy: 0.4197\n",
            "Epoch 134/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.7952 - accuracy: 0.4224\n",
            "Epoch 135/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.7827 - accuracy: 0.4256\n",
            "Epoch 136/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.7694 - accuracy: 0.4271\n",
            "Epoch 137/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.7528 - accuracy: 0.4304\n",
            "Epoch 138/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.7407 - accuracy: 0.4290\n",
            "Epoch 139/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 2.7267 - accuracy: 0.4339\n",
            "Epoch 140/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.7116 - accuracy: 0.4387\n",
            "Epoch 141/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.6955 - accuracy: 0.4415\n",
            "Epoch 142/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.6838 - accuracy: 0.4413\n",
            "Epoch 143/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 2.6692 - accuracy: 0.4428\n",
            "Epoch 144/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.6556 - accuracy: 0.4475\n",
            "Epoch 145/650\n",
            "1069/1069 [==============================] - 17s 15ms/step - loss: 2.6427 - accuracy: 0.4488\n",
            "Epoch 146/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 2.6297 - accuracy: 0.4520\n",
            "Epoch 147/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.6143 - accuracy: 0.4564\n",
            "Epoch 148/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.6007 - accuracy: 0.4592\n",
            "Epoch 149/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.5883 - accuracy: 0.4587\n",
            "Epoch 150/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.5746 - accuracy: 0.4637\n",
            "Epoch 151/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.5629 - accuracy: 0.4654\n",
            "Epoch 152/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.5490 - accuracy: 0.4706\n",
            "Epoch 153/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 2.5373 - accuracy: 0.4698\n",
            "Epoch 154/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.5225 - accuracy: 0.4739\n",
            "Epoch 155/650\n",
            "1069/1069 [==============================] - 17s 15ms/step - loss: 2.5114 - accuracy: 0.4772\n",
            "Epoch 156/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.4973 - accuracy: 0.4793\n",
            "Epoch 157/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 2.4884 - accuracy: 0.4812\n",
            "Epoch 158/650\n",
            "1069/1069 [==============================] - 17s 15ms/step - loss: 2.4733 - accuracy: 0.4834\n",
            "Epoch 159/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.4608 - accuracy: 0.4859\n",
            "Epoch 160/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 2.4474 - accuracy: 0.4887\n",
            "Epoch 161/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.4369 - accuracy: 0.4911\n",
            "Epoch 162/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.4241 - accuracy: 0.4946\n",
            "Epoch 163/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 2.4118 - accuracy: 0.4955\n",
            "Epoch 164/650\n",
            "1069/1069 [==============================] - 17s 15ms/step - loss: 2.3999 - accuracy: 0.4992\n",
            "Epoch 165/650\n",
            "1069/1069 [==============================] - 17s 15ms/step - loss: 2.3883 - accuracy: 0.4993\n",
            "Epoch 166/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.3777 - accuracy: 0.5037\n",
            "Epoch 167/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 2.3657 - accuracy: 0.5066\n",
            "Epoch 168/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.3525 - accuracy: 0.5100\n",
            "Epoch 169/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.3409 - accuracy: 0.5123\n",
            "Epoch 170/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 2.3307 - accuracy: 0.5120\n",
            "Epoch 171/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 2.3215 - accuracy: 0.5163\n",
            "Epoch 172/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.3063 - accuracy: 0.5181\n",
            "Epoch 173/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.2961 - accuracy: 0.5214\n",
            "Epoch 174/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.2847 - accuracy: 0.5238\n",
            "Epoch 175/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.2737 - accuracy: 0.5276\n",
            "Epoch 176/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.2644 - accuracy: 0.5280\n",
            "Epoch 177/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 2.2513 - accuracy: 0.5323\n",
            "Epoch 178/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.2431 - accuracy: 0.5322\n",
            "Epoch 179/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.2318 - accuracy: 0.5373\n",
            "Epoch 180/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 2.2189 - accuracy: 0.5358\n",
            "Epoch 181/650\n",
            "1069/1069 [==============================] - 17s 15ms/step - loss: 2.2109 - accuracy: 0.5421\n",
            "Epoch 182/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.2001 - accuracy: 0.5422\n",
            "Epoch 183/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 2.1906 - accuracy: 0.5441\n",
            "Epoch 184/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.1799 - accuracy: 0.5475\n",
            "Epoch 185/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 2.1700 - accuracy: 0.5499\n",
            "Epoch 186/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.1580 - accuracy: 0.5525\n",
            "Epoch 187/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.1502 - accuracy: 0.5534\n",
            "Epoch 188/650\n",
            "1069/1069 [==============================] - 17s 15ms/step - loss: 2.1380 - accuracy: 0.5570\n",
            "Epoch 189/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.1309 - accuracy: 0.5585\n",
            "Epoch 190/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 2.1194 - accuracy: 0.5599\n",
            "Epoch 191/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.1103 - accuracy: 0.5620\n",
            "Epoch 192/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.1005 - accuracy: 0.5621\n",
            "Epoch 193/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 2.0904 - accuracy: 0.5666\n",
            "Epoch 194/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.0815 - accuracy: 0.5677\n",
            "Epoch 195/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.0704 - accuracy: 0.5694\n",
            "Epoch 196/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.0626 - accuracy: 0.5723\n",
            "Epoch 197/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 2.0540 - accuracy: 0.5760\n",
            "Epoch 198/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 2.0404 - accuracy: 0.5782\n",
            "Epoch 199/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 2.0323 - accuracy: 0.5791\n",
            "Epoch 200/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 2.0249 - accuracy: 0.5824\n",
            "Epoch 201/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.0140 - accuracy: 0.5821\n",
            "Epoch 202/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 2.0066 - accuracy: 0.5838\n",
            "Epoch 203/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.9950 - accuracy: 0.5884\n",
            "Epoch 204/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 1.9901 - accuracy: 0.5866\n",
            "Epoch 205/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 1.9792 - accuracy: 0.5935\n",
            "Epoch 206/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.9719 - accuracy: 0.5922\n",
            "Epoch 207/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.9615 - accuracy: 0.5941\n",
            "Epoch 208/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 1.9521 - accuracy: 0.5974\n",
            "Epoch 209/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.9449 - accuracy: 0.5993\n",
            "Epoch 210/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.9353 - accuracy: 0.5996\n",
            "Epoch 211/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.9267 - accuracy: 0.6039\n",
            "Epoch 212/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.9182 - accuracy: 0.6038\n",
            "Epoch 213/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.9083 - accuracy: 0.6073\n",
            "Epoch 214/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.9012 - accuracy: 0.6094\n",
            "Epoch 215/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.8931 - accuracy: 0.6082\n",
            "Epoch 216/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.8858 - accuracy: 0.6097\n",
            "Epoch 217/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.8773 - accuracy: 0.6140\n",
            "Epoch 218/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.8696 - accuracy: 0.6135\n",
            "Epoch 219/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.8590 - accuracy: 0.6176\n",
            "Epoch 220/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.8521 - accuracy: 0.6188\n",
            "Epoch 221/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 1.8434 - accuracy: 0.6233\n",
            "Epoch 222/650\n",
            "1069/1069 [==============================] - 17s 15ms/step - loss: 1.8359 - accuracy: 0.6228\n",
            "Epoch 223/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.8280 - accuracy: 0.6249\n",
            "Epoch 224/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 1.8223 - accuracy: 0.6258\n",
            "Epoch 225/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 1.8125 - accuracy: 0.6277\n",
            "Epoch 226/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.8040 - accuracy: 0.6304\n",
            "Epoch 227/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.7979 - accuracy: 0.6312\n",
            "Epoch 228/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.7914 - accuracy: 0.6338\n",
            "Epoch 229/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.7825 - accuracy: 0.6345\n",
            "Epoch 230/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.7752 - accuracy: 0.6347\n",
            "Epoch 231/650\n",
            "1069/1069 [==============================] - 17s 15ms/step - loss: 1.7662 - accuracy: 0.6389\n",
            "Epoch 232/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 1.7587 - accuracy: 0.6417\n",
            "Epoch 233/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.7531 - accuracy: 0.6416\n",
            "Epoch 234/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 1.7462 - accuracy: 0.6448\n",
            "Epoch 235/650\n",
            "1069/1069 [==============================] - 17s 15ms/step - loss: 1.7372 - accuracy: 0.6444\n",
            "Epoch 236/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.7298 - accuracy: 0.6494\n",
            "Epoch 237/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.7220 - accuracy: 0.6504\n",
            "Epoch 238/650\n",
            "1069/1069 [==============================] - 17s 15ms/step - loss: 1.7162 - accuracy: 0.6504\n",
            "Epoch 239/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.7074 - accuracy: 0.6521\n",
            "Epoch 240/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.7005 - accuracy: 0.6546\n",
            "Epoch 241/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.6940 - accuracy: 0.6554\n",
            "Epoch 242/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.6872 - accuracy: 0.6578\n",
            "Epoch 243/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.6781 - accuracy: 0.6574\n",
            "Epoch 244/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.6729 - accuracy: 0.6614\n",
            "Epoch 245/650\n",
            "1069/1069 [==============================] - 17s 15ms/step - loss: 1.6664 - accuracy: 0.6611\n",
            "Epoch 246/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.6592 - accuracy: 0.6645\n",
            "Epoch 247/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.6517 - accuracy: 0.6631\n",
            "Epoch 248/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.6449 - accuracy: 0.6656\n",
            "Epoch 249/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.6379 - accuracy: 0.6679\n",
            "Epoch 250/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.6289 - accuracy: 0.6701\n",
            "Epoch 251/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.6271 - accuracy: 0.6693\n",
            "Epoch 252/650\n",
            "1069/1069 [==============================] - 20s 19ms/step - loss: 1.6187 - accuracy: 0.6716\n",
            "Epoch 253/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.6086 - accuracy: 0.6777\n",
            "Epoch 254/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.6034 - accuracy: 0.6757\n",
            "Epoch 255/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.5966 - accuracy: 0.6780\n",
            "Epoch 256/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 1.5922 - accuracy: 0.6788\n",
            "Epoch 257/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.5864 - accuracy: 0.6782\n",
            "Epoch 258/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.5751 - accuracy: 0.6842\n",
            "Epoch 259/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.5738 - accuracy: 0.6834\n",
            "Epoch 260/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.5666 - accuracy: 0.6826\n",
            "Epoch 261/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.5579 - accuracy: 0.6899\n",
            "Epoch 262/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.5525 - accuracy: 0.6870\n",
            "Epoch 263/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.5482 - accuracy: 0.6888\n",
            "Epoch 264/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.5396 - accuracy: 0.6897\n",
            "Epoch 265/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 1.5319 - accuracy: 0.6929\n",
            "Epoch 266/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 1.5293 - accuracy: 0.6943\n",
            "Epoch 267/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 1.5186 - accuracy: 0.6984\n",
            "Epoch 268/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.5159 - accuracy: 0.6957\n",
            "Epoch 269/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 1.5074 - accuracy: 0.6997\n",
            "Epoch 270/650\n",
            "1069/1069 [==============================] - 17s 15ms/step - loss: 1.5026 - accuracy: 0.6973\n",
            "Epoch 271/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.4980 - accuracy: 0.7010\n",
            "Epoch 272/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.4912 - accuracy: 0.7025\n",
            "Epoch 273/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.4854 - accuracy: 0.7035\n",
            "Epoch 274/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 1.4789 - accuracy: 0.7071\n",
            "Epoch 275/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.4735 - accuracy: 0.7055\n",
            "Epoch 276/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.4674 - accuracy: 0.7059\n",
            "Epoch 277/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.4622 - accuracy: 0.7084\n",
            "Epoch 278/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.4571 - accuracy: 0.7090\n",
            "Epoch 279/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.4518 - accuracy: 0.7117\n",
            "Epoch 280/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.4449 - accuracy: 0.7142\n",
            "Epoch 281/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.4364 - accuracy: 0.7132\n",
            "Epoch 282/650\n",
            "1069/1069 [==============================] - 17s 15ms/step - loss: 1.4329 - accuracy: 0.7150\n",
            "Epoch 283/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 1.4282 - accuracy: 0.7164\n",
            "Epoch 284/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.4238 - accuracy: 0.7179\n",
            "Epoch 285/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.4149 - accuracy: 0.7196\n",
            "Epoch 286/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.4116 - accuracy: 0.7210\n",
            "Epoch 287/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.4040 - accuracy: 0.7221\n",
            "Epoch 288/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.4007 - accuracy: 0.7233\n",
            "Epoch 289/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.3951 - accuracy: 0.7266\n",
            "Epoch 290/650\n",
            "1069/1069 [==============================] - 16s 15ms/step - loss: 1.3866 - accuracy: 0.7269\n",
            "Epoch 291/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.3850 - accuracy: 0.7263\n",
            "Epoch 292/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.3758 - accuracy: 0.7303\n",
            "Epoch 293/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.3725 - accuracy: 0.7285\n",
            "Epoch 294/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.3672 - accuracy: 0.7295\n",
            "Epoch 295/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.3614 - accuracy: 0.7321\n",
            "Epoch 296/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.3564 - accuracy: 0.7323\n",
            "Epoch 297/650\n",
            "1069/1069 [==============================] - 17s 15ms/step - loss: 1.3537 - accuracy: 0.7328\n",
            "Epoch 298/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 1.3463 - accuracy: 0.7353\n",
            "Epoch 299/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.3429 - accuracy: 0.7379\n",
            "Epoch 300/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.3352 - accuracy: 0.7393\n",
            "Epoch 301/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.3326 - accuracy: 0.7380\n",
            "Epoch 302/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 1.3243 - accuracy: 0.7390\n",
            "Epoch 303/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.3211 - accuracy: 0.7414\n",
            "Epoch 304/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.3168 - accuracy: 0.7422\n",
            "Epoch 305/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 1.3099 - accuracy: 0.7429\n",
            "Epoch 306/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.3082 - accuracy: 0.7452\n",
            "Epoch 307/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.3011 - accuracy: 0.7457\n",
            "Epoch 308/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.2980 - accuracy: 0.7461\n",
            "Epoch 309/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.2901 - accuracy: 0.7475\n",
            "Epoch 310/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.2879 - accuracy: 0.7476\n",
            "Epoch 311/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.2808 - accuracy: 0.7514\n",
            "Epoch 312/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.2777 - accuracy: 0.7505\n",
            "Epoch 313/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.2700 - accuracy: 0.7536\n",
            "Epoch 314/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.2670 - accuracy: 0.7517\n",
            "Epoch 315/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.2645 - accuracy: 0.7532\n",
            "Epoch 316/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.2571 - accuracy: 0.7569\n",
            "Epoch 317/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.2530 - accuracy: 0.7555\n",
            "Epoch 318/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.2491 - accuracy: 0.7591\n",
            "Epoch 319/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 1.2434 - accuracy: 0.7596\n",
            "Epoch 320/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.2348 - accuracy: 0.7622\n",
            "Epoch 321/650\n",
            "1069/1069 [==============================] - 17s 15ms/step - loss: 1.2372 - accuracy: 0.7614\n",
            "Epoch 322/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.2317 - accuracy: 0.7607\n",
            "Epoch 323/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.2256 - accuracy: 0.7637\n",
            "Epoch 324/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.2202 - accuracy: 0.7648\n",
            "Epoch 325/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 1.2178 - accuracy: 0.7659\n",
            "Epoch 326/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.2127 - accuracy: 0.7660\n",
            "Epoch 327/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.2038 - accuracy: 0.7694\n",
            "Epoch 328/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.2026 - accuracy: 0.7683\n",
            "Epoch 329/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.1992 - accuracy: 0.7679\n",
            "Epoch 330/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.1939 - accuracy: 0.7725\n",
            "Epoch 331/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.1900 - accuracy: 0.7714\n",
            "Epoch 332/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 1.1868 - accuracy: 0.7715\n",
            "Epoch 333/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.1828 - accuracy: 0.7731\n",
            "Epoch 334/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.1762 - accuracy: 0.7751\n",
            "Epoch 335/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.1734 - accuracy: 0.7761\n",
            "Epoch 336/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.1705 - accuracy: 0.7767\n",
            "Epoch 337/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.1625 - accuracy: 0.7771\n",
            "Epoch 338/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.1600 - accuracy: 0.7774\n",
            "Epoch 339/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.1572 - accuracy: 0.7797\n",
            "Epoch 340/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.1513 - accuracy: 0.7805\n",
            "Epoch 341/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.1463 - accuracy: 0.7829\n",
            "Epoch 342/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.1417 - accuracy: 0.7845\n",
            "Epoch 343/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.1381 - accuracy: 0.7829\n",
            "Epoch 344/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.1351 - accuracy: 0.7850\n",
            "Epoch 345/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.1311 - accuracy: 0.7866\n",
            "Epoch 346/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.1272 - accuracy: 0.7872\n",
            "Epoch 347/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.1247 - accuracy: 0.7877\n",
            "Epoch 348/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.1182 - accuracy: 0.7877\n",
            "Epoch 349/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.1157 - accuracy: 0.7908\n",
            "Epoch 350/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.1103 - accuracy: 0.7922\n",
            "Epoch 351/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.1100 - accuracy: 0.7907\n",
            "Epoch 352/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.1030 - accuracy: 0.7955\n",
            "Epoch 353/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.0984 - accuracy: 0.7940\n",
            "Epoch 354/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.0936 - accuracy: 0.7938\n",
            "Epoch 355/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.0898 - accuracy: 0.7958\n",
            "Epoch 356/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.0876 - accuracy: 0.7963\n",
            "Epoch 357/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 1.0840 - accuracy: 0.7971\n",
            "Epoch 358/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.0802 - accuracy: 0.7974\n",
            "Epoch 359/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.0772 - accuracy: 0.7979\n",
            "Epoch 360/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.0714 - accuracy: 0.8019\n",
            "Epoch 361/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 1.0691 - accuracy: 0.8000\n",
            "Epoch 362/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.0645 - accuracy: 0.8011\n",
            "Epoch 363/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.0644 - accuracy: 0.8005\n",
            "Epoch 364/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 1.0559 - accuracy: 0.8040\n",
            "Epoch 365/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.0509 - accuracy: 0.8032\n",
            "Epoch 366/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.0512 - accuracy: 0.8054\n",
            "Epoch 367/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.0476 - accuracy: 0.8033\n",
            "Epoch 368/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.0442 - accuracy: 0.8069\n",
            "Epoch 369/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.0398 - accuracy: 0.8061\n",
            "Epoch 370/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 1.0364 - accuracy: 0.8091\n",
            "Epoch 371/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 1.0261 - accuracy: 0.8097\n",
            "Epoch 372/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.0253 - accuracy: 0.8112\n",
            "Epoch 373/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.0253 - accuracy: 0.8112\n",
            "Epoch 374/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 1.0211 - accuracy: 0.8116\n",
            "Epoch 375/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.0173 - accuracy: 0.8125\n",
            "Epoch 376/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.0147 - accuracy: 0.8134\n",
            "Epoch 377/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 1.0072 - accuracy: 0.8150\n",
            "Epoch 378/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.0073 - accuracy: 0.8150\n",
            "Epoch 379/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 1.0062 - accuracy: 0.8135\n",
            "Epoch 380/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 1.0015 - accuracy: 0.8169\n",
            "Epoch 381/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.9958 - accuracy: 0.8173\n",
            "Epoch 382/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.9930 - accuracy: 0.8207\n",
            "Epoch 383/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 0.9859 - accuracy: 0.8204\n",
            "Epoch 384/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.9848 - accuracy: 0.8201\n",
            "Epoch 385/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 0.9837 - accuracy: 0.8204\n",
            "Epoch 386/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.9774 - accuracy: 0.8228\n",
            "Epoch 387/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.9735 - accuracy: 0.8216\n",
            "Epoch 388/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 0.9756 - accuracy: 0.8225\n",
            "Epoch 389/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.9691 - accuracy: 0.8249\n",
            "Epoch 390/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.9618 - accuracy: 0.8256\n",
            "Epoch 391/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.9611 - accuracy: 0.8260\n",
            "Epoch 392/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.9597 - accuracy: 0.8252\n",
            "Epoch 393/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.9553 - accuracy: 0.8281\n",
            "Epoch 394/650\n",
            "1069/1069 [==============================] - 21s 19ms/step - loss: 0.9516 - accuracy: 0.8293\n",
            "Epoch 395/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.9492 - accuracy: 0.8295\n",
            "Epoch 396/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.9511 - accuracy: 0.8263\n",
            "Epoch 397/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.9417 - accuracy: 0.8315\n",
            "Epoch 398/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.9399 - accuracy: 0.8310\n",
            "Epoch 399/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.9347 - accuracy: 0.8319\n",
            "Epoch 400/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.9348 - accuracy: 0.8304\n",
            "Epoch 401/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.9343 - accuracy: 0.8324\n",
            "Epoch 402/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.9277 - accuracy: 0.8343\n",
            "Epoch 403/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.9229 - accuracy: 0.8359\n",
            "Epoch 404/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.9219 - accuracy: 0.8359\n",
            "Epoch 405/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.9197 - accuracy: 0.8349\n",
            "Epoch 406/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.9157 - accuracy: 0.8367\n",
            "Epoch 407/650\n",
            "1069/1069 [==============================] - 20s 19ms/step - loss: 0.9146 - accuracy: 0.8359\n",
            "Epoch 408/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.9078 - accuracy: 0.8373\n",
            "Epoch 409/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.9046 - accuracy: 0.8385\n",
            "Epoch 410/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.9071 - accuracy: 0.8378\n",
            "Epoch 411/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.9014 - accuracy: 0.8397\n",
            "Epoch 412/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.8952 - accuracy: 0.8419\n",
            "Epoch 413/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.8935 - accuracy: 0.8395\n",
            "Epoch 414/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.8914 - accuracy: 0.8429\n",
            "Epoch 415/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 0.8877 - accuracy: 0.8419\n",
            "Epoch 416/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.8872 - accuracy: 0.8422\n",
            "Epoch 417/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.8838 - accuracy: 0.8425\n",
            "Epoch 418/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 0.8794 - accuracy: 0.8439\n",
            "Epoch 419/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.8756 - accuracy: 0.8452\n",
            "Epoch 420/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.8746 - accuracy: 0.8454\n",
            "Epoch 421/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.8730 - accuracy: 0.8444\n",
            "Epoch 422/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.8654 - accuracy: 0.8488\n",
            "Epoch 423/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.8628 - accuracy: 0.8457\n",
            "Epoch 424/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.8665 - accuracy: 0.8470\n",
            "Epoch 425/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.8626 - accuracy: 0.8480\n",
            "Epoch 426/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.8569 - accuracy: 0.8493\n",
            "Epoch 427/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.8560 - accuracy: 0.8483\n",
            "Epoch 428/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.8488 - accuracy: 0.8496\n",
            "Epoch 429/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.8476 - accuracy: 0.8511\n",
            "Epoch 430/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.8483 - accuracy: 0.8512\n",
            "Epoch 431/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.8439 - accuracy: 0.8521\n",
            "Epoch 432/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.8397 - accuracy: 0.8530\n",
            "Epoch 433/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.8381 - accuracy: 0.8517\n",
            "Epoch 434/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.8360 - accuracy: 0.8532\n",
            "Epoch 435/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.8308 - accuracy: 0.8549\n",
            "Epoch 436/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.8324 - accuracy: 0.8526\n",
            "Epoch 437/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.8278 - accuracy: 0.8539\n",
            "Epoch 438/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.8236 - accuracy: 0.8553\n",
            "Epoch 439/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.8204 - accuracy: 0.8559\n",
            "Epoch 440/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.8209 - accuracy: 0.8561\n",
            "Epoch 441/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.8146 - accuracy: 0.8579\n",
            "Epoch 442/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 0.8110 - accuracy: 0.8599\n",
            "Epoch 443/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.8118 - accuracy: 0.8580\n",
            "Epoch 444/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.8074 - accuracy: 0.8584\n",
            "Epoch 445/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.8039 - accuracy: 0.8600\n",
            "Epoch 446/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 0.8048 - accuracy: 0.8576\n",
            "Epoch 447/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.8008 - accuracy: 0.8591\n",
            "Epoch 448/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.7972 - accuracy: 0.8620\n",
            "Epoch 449/650\n",
            "1069/1069 [==============================] - 20s 18ms/step - loss: 0.7977 - accuracy: 0.8604\n",
            "Epoch 450/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.7955 - accuracy: 0.8617\n",
            "Epoch 451/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.7933 - accuracy: 0.8616\n",
            "Epoch 452/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 0.7884 - accuracy: 0.8629\n",
            "Epoch 453/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.7868 - accuracy: 0.8639\n",
            "Epoch 454/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.7834 - accuracy: 0.8642\n",
            "Epoch 455/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.7807 - accuracy: 0.8651\n",
            "Epoch 456/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.7746 - accuracy: 0.8661\n",
            "Epoch 457/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.7739 - accuracy: 0.8660\n",
            "Epoch 458/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.7741 - accuracy: 0.8670\n",
            "Epoch 459/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.7743 - accuracy: 0.8656\n",
            "Epoch 460/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.7688 - accuracy: 0.8672\n",
            "Epoch 461/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.7650 - accuracy: 0.8677\n",
            "Epoch 462/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.7608 - accuracy: 0.8694\n",
            "Epoch 463/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.7628 - accuracy: 0.8695\n",
            "Epoch 464/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 0.7590 - accuracy: 0.8689\n",
            "Epoch 465/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.7564 - accuracy: 0.8697\n",
            "Epoch 466/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.7518 - accuracy: 0.8725\n",
            "Epoch 467/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.7518 - accuracy: 0.8731\n",
            "Epoch 468/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.7513 - accuracy: 0.8700\n",
            "Epoch 469/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.7471 - accuracy: 0.8736\n",
            "Epoch 470/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.7450 - accuracy: 0.8718\n",
            "Epoch 471/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.7408 - accuracy: 0.8718\n",
            "Epoch 472/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.7439 - accuracy: 0.8720\n",
            "Epoch 473/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 0.7405 - accuracy: 0.8711\n",
            "Epoch 474/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.7364 - accuracy: 0.8735\n",
            "Epoch 475/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.7325 - accuracy: 0.8759\n",
            "Epoch 476/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 0.7298 - accuracy: 0.8749\n",
            "Epoch 477/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.7312 - accuracy: 0.8737\n",
            "Epoch 478/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.7246 - accuracy: 0.8762\n",
            "Epoch 479/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.7239 - accuracy: 0.8767\n",
            "Epoch 480/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.7207 - accuracy: 0.8770\n",
            "Epoch 481/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.7205 - accuracy: 0.8780\n",
            "Epoch 482/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.7177 - accuracy: 0.8780\n",
            "Epoch 483/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.7132 - accuracy: 0.8794\n",
            "Epoch 484/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.7150 - accuracy: 0.8780\n",
            "Epoch 485/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.7075 - accuracy: 0.8808\n",
            "Epoch 486/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.7082 - accuracy: 0.8802\n",
            "Epoch 487/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.7067 - accuracy: 0.8814\n",
            "Epoch 488/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.7082 - accuracy: 0.8796\n",
            "Epoch 489/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.7052 - accuracy: 0.8785\n",
            "Epoch 490/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.6981 - accuracy: 0.8812\n",
            "Epoch 491/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.6977 - accuracy: 0.8818\n",
            "Epoch 492/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.6946 - accuracy: 0.8830\n",
            "Epoch 493/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.6948 - accuracy: 0.8828\n",
            "Epoch 494/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.6905 - accuracy: 0.8836\n",
            "Epoch 495/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.6861 - accuracy: 0.8846\n",
            "Epoch 496/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.6854 - accuracy: 0.8846\n",
            "Epoch 497/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.6855 - accuracy: 0.8837\n",
            "Epoch 498/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.6800 - accuracy: 0.8870\n",
            "Epoch 499/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.6837 - accuracy: 0.8846\n",
            "Epoch 500/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.6765 - accuracy: 0.8856\n",
            "Epoch 501/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.6782 - accuracy: 0.8871\n",
            "Epoch 502/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.6753 - accuracy: 0.8866\n",
            "Epoch 503/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.6720 - accuracy: 0.8857\n",
            "Epoch 504/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.6681 - accuracy: 0.8878\n",
            "Epoch 505/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.6707 - accuracy: 0.8874\n",
            "Epoch 506/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.6679 - accuracy: 0.8882\n",
            "Epoch 507/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.6654 - accuracy: 0.8880\n",
            "Epoch 508/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.6597 - accuracy: 0.8898\n",
            "Epoch 509/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.6606 - accuracy: 0.8894\n",
            "Epoch 510/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.6576 - accuracy: 0.8905\n",
            "Epoch 511/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.6558 - accuracy: 0.8916\n",
            "Epoch 512/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.6520 - accuracy: 0.8931\n",
            "Epoch 513/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.6543 - accuracy: 0.8914\n",
            "Epoch 514/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.6493 - accuracy: 0.8937\n",
            "Epoch 515/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.6482 - accuracy: 0.8928\n",
            "Epoch 516/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.6467 - accuracy: 0.8929\n",
            "Epoch 517/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.6434 - accuracy: 0.8943\n",
            "Epoch 518/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.6454 - accuracy: 0.8934\n",
            "Epoch 519/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.6405 - accuracy: 0.8939\n",
            "Epoch 520/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.6394 - accuracy: 0.8949\n",
            "Epoch 521/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.6326 - accuracy: 0.8964\n",
            "Epoch 522/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 0.6372 - accuracy: 0.8952\n",
            "Epoch 523/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.6341 - accuracy: 0.8942\n",
            "Epoch 524/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.6277 - accuracy: 0.8966\n",
            "Epoch 525/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.6294 - accuracy: 0.8974\n",
            "Epoch 526/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.6284 - accuracy: 0.8964\n",
            "Epoch 527/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.6290 - accuracy: 0.8952\n",
            "Epoch 528/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.6215 - accuracy: 0.8997\n",
            "Epoch 529/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.6192 - accuracy: 0.8988\n",
            "Epoch 530/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.6202 - accuracy: 0.8980\n",
            "Epoch 531/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.6162 - accuracy: 0.8990\n",
            "Epoch 532/650\n",
            "1069/1069 [==============================] - 17s 16ms/step - loss: 0.6181 - accuracy: 0.8983\n",
            "Epoch 533/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.6142 - accuracy: 0.9001\n",
            "Epoch 534/650\n",
            "1069/1069 [==============================] - 21s 19ms/step - loss: 0.6103 - accuracy: 0.9013\n",
            "Epoch 535/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.6086 - accuracy: 0.9008\n",
            "Epoch 536/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.6069 - accuracy: 0.9006\n",
            "Epoch 537/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.6066 - accuracy: 0.9016\n",
            "Epoch 538/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.6053 - accuracy: 0.9000\n",
            "Epoch 539/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.6032 - accuracy: 0.9005\n",
            "Epoch 540/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.6024 - accuracy: 0.9014\n",
            "Epoch 541/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.5976 - accuracy: 0.9038\n",
            "Epoch 542/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.5987 - accuracy: 0.9038\n",
            "Epoch 543/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5948 - accuracy: 0.9024\n",
            "Epoch 544/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5927 - accuracy: 0.9035\n",
            "Epoch 545/650\n",
            "1069/1069 [==============================] - 21s 20ms/step - loss: 0.5961 - accuracy: 0.9032\n",
            "Epoch 546/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5941 - accuracy: 0.9032\n",
            "Epoch 547/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5885 - accuracy: 0.9057\n",
            "Epoch 548/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.5901 - accuracy: 0.9027\n",
            "Epoch 549/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.5828 - accuracy: 0.9052\n",
            "Epoch 550/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5825 - accuracy: 0.9049\n",
            "Epoch 551/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 0.5849 - accuracy: 0.9034\n",
            "Epoch 552/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.5797 - accuracy: 0.9059\n",
            "Epoch 553/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5783 - accuracy: 0.9080\n",
            "Epoch 554/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 0.5763 - accuracy: 0.9059\n",
            "Epoch 555/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5758 - accuracy: 0.9079\n",
            "Epoch 556/650\n",
            "1069/1069 [==============================] - 21s 20ms/step - loss: 0.5754 - accuracy: 0.9068\n",
            "Epoch 557/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5714 - accuracy: 0.9073\n",
            "Epoch 558/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5669 - accuracy: 0.9080\n",
            "Epoch 559/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.5700 - accuracy: 0.9087\n",
            "Epoch 560/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5672 - accuracy: 0.9091\n",
            "Epoch 561/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5679 - accuracy: 0.9080\n",
            "Epoch 562/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.5616 - accuracy: 0.9109\n",
            "Epoch 563/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5615 - accuracy: 0.9096\n",
            "Epoch 564/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5603 - accuracy: 0.9098\n",
            "Epoch 565/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.5561 - accuracy: 0.9106\n",
            "Epoch 566/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5566 - accuracy: 0.9102\n",
            "Epoch 567/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.5573 - accuracy: 0.9116\n",
            "Epoch 568/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.5529 - accuracy: 0.9104\n",
            "Epoch 569/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.5481 - accuracy: 0.9125\n",
            "Epoch 570/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5475 - accuracy: 0.9124\n",
            "Epoch 571/650\n",
            "1069/1069 [==============================] - 20s 18ms/step - loss: 0.5512 - accuracy: 0.9138\n",
            "Epoch 572/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5476 - accuracy: 0.9130\n",
            "Epoch 573/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5464 - accuracy: 0.9129\n",
            "Epoch 574/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.5464 - accuracy: 0.9132\n",
            "Epoch 575/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5414 - accuracy: 0.9143\n",
            "Epoch 576/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5388 - accuracy: 0.9161\n",
            "Epoch 577/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.5363 - accuracy: 0.9154\n",
            "Epoch 578/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.5378 - accuracy: 0.9137\n",
            "Epoch 579/650\n",
            "1069/1069 [==============================] - 19s 17ms/step - loss: 0.5342 - accuracy: 0.9168\n",
            "Epoch 580/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5335 - accuracy: 0.9158\n",
            "Epoch 581/650\n",
            "1069/1069 [==============================] - 18s 16ms/step - loss: 0.5381 - accuracy: 0.9136\n",
            "Epoch 582/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5284 - accuracy: 0.9175\n",
            "Epoch 583/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.5271 - accuracy: 0.9167\n",
            "Epoch 584/650\n",
            "1069/1069 [==============================] - 18s 17ms/step - loss: 0.5264 - accuracy: 0.9167\n",
            "Epoch 585/650\n",
            "1069/1069 [==============================] - 19s 18ms/step - loss: 0.5280 - accuracy: 0.9166\n",
            "Epoch 586/650\n",
            "1069/1069 [==============================] - 20s 19ms/step - loss: 0.5247 - accuracy: 0.9166\n",
            "Epoch 587/650\n",
            "1069/1069 [==============================] - 23s 22ms/step - loss: 0.5242 - accuracy: 0.9167\n",
            "Epoch 588/650\n",
            "1069/1069 [==============================] - 24s 22ms/step - loss: 0.5212 - accuracy: 0.9183\n",
            "Epoch 589/650\n",
            "1069/1069 [==============================] - 22s 21ms/step - loss: 0.5192 - accuracy: 0.9190\n",
            "Epoch 590/650\n",
            "1069/1069 [==============================] - 23s 21ms/step - loss: 0.5186 - accuracy: 0.9192\n",
            "Epoch 591/650\n",
            "1069/1069 [==============================] - 21s 20ms/step - loss: 0.5186 - accuracy: 0.9188\n",
            "Epoch 592/650\n",
            "1069/1069 [==============================] - 23s 21ms/step - loss: 0.5182 - accuracy: 0.9185\n",
            "Epoch 593/650\n",
            "1069/1069 [==============================] - 23s 22ms/step - loss: 0.5146 - accuracy: 0.9183\n",
            "Epoch 594/650\n",
            "1069/1069 [==============================] - 23s 22ms/step - loss: 0.5123 - accuracy: 0.9198\n",
            "Epoch 595/650\n",
            "1069/1069 [==============================] - 24s 23ms/step - loss: 0.5124 - accuracy: 0.9178\n",
            "Epoch 596/650\n",
            "1069/1069 [==============================] - 30s 28ms/step - loss: 0.5113 - accuracy: 0.9197\n",
            "Epoch 597/650\n",
            "1069/1069 [==============================] - 26s 24ms/step - loss: 0.5086 - accuracy: 0.9201\n",
            "Epoch 598/650\n",
            "1069/1069 [==============================] - 24s 22ms/step - loss: 0.5082 - accuracy: 0.9197\n",
            "Epoch 599/650\n",
            "1069/1069 [==============================] - 24s 23ms/step - loss: 0.5073 - accuracy: 0.9207\n",
            "Epoch 600/650\n",
            "1069/1069 [==============================] - 25s 23ms/step - loss: 0.5027 - accuracy: 0.9213\n",
            "Epoch 601/650\n",
            "1069/1069 [==============================] - 25s 23ms/step - loss: 0.5010 - accuracy: 0.9225\n",
            "Epoch 602/650\n",
            "1069/1069 [==============================] - 24s 22ms/step - loss: 0.5014 - accuracy: 0.9224\n",
            "Epoch 603/650\n",
            "1069/1069 [==============================] - 24s 22ms/step - loss: 0.4985 - accuracy: 0.9218\n",
            "Epoch 604/650\n",
            "1069/1069 [==============================] - 27s 25ms/step - loss: 0.4997 - accuracy: 0.9211\n",
            "Epoch 605/650\n",
            "1069/1069 [==============================] - 26s 24ms/step - loss: 0.4956 - accuracy: 0.9240\n",
            "Epoch 606/650\n",
            "1069/1069 [==============================] - 25s 24ms/step - loss: 0.4957 - accuracy: 0.9218\n",
            "Epoch 607/650\n",
            "1069/1069 [==============================] - 25s 24ms/step - loss: 0.4950 - accuracy: 0.9223\n",
            "Epoch 608/650\n",
            "1069/1069 [==============================] - 29s 28ms/step - loss: 0.4887 - accuracy: 0.9243\n",
            "Epoch 609/650\n",
            "1069/1069 [==============================] - 28s 26ms/step - loss: 0.4906 - accuracy: 0.9240\n",
            "Epoch 610/650\n",
            "1069/1069 [==============================] - 27s 26ms/step - loss: 0.4880 - accuracy: 0.9239\n",
            "Epoch 611/650\n",
            "1069/1069 [==============================] - 29s 27ms/step - loss: 0.4908 - accuracy: 0.9235\n",
            "Epoch 612/650\n",
            "1069/1069 [==============================] - 28s 26ms/step - loss: 0.4862 - accuracy: 0.9243\n",
            "Epoch 613/650\n",
            "1069/1069 [==============================] - 29s 27ms/step - loss: 0.4839 - accuracy: 0.9253\n",
            "Epoch 614/650\n",
            "1069/1069 [==============================] - 29s 27ms/step - loss: 0.4857 - accuracy: 0.9250\n",
            "Epoch 615/650\n",
            "1069/1069 [==============================] - 29s 27ms/step - loss: 0.4844 - accuracy: 0.9253\n",
            "Epoch 616/650\n",
            "1069/1069 [==============================] - 28s 27ms/step - loss: 0.4796 - accuracy: 0.9267\n",
            "Epoch 617/650\n",
            "1069/1069 [==============================] - 28s 26ms/step - loss: 0.4803 - accuracy: 0.9253\n",
            "Epoch 618/650\n",
            "1069/1069 [==============================] - 30s 28ms/step - loss: 0.4827 - accuracy: 0.9243\n",
            "Epoch 619/650\n",
            "1069/1069 [==============================] - 29s 27ms/step - loss: 0.4785 - accuracy: 0.9261\n",
            "Epoch 620/650\n",
            "1069/1069 [==============================] - 28s 26ms/step - loss: 0.4757 - accuracy: 0.9277\n",
            "Epoch 621/650\n",
            "1069/1069 [==============================] - 28s 26ms/step - loss: 0.4742 - accuracy: 0.9260\n",
            "Epoch 622/650\n",
            "1069/1069 [==============================] - 28s 26ms/step - loss: 0.4730 - accuracy: 0.9268\n",
            "Epoch 623/650\n",
            "1069/1069 [==============================] - 28s 26ms/step - loss: 0.4728 - accuracy: 0.9266\n",
            "Epoch 624/650\n",
            "1069/1069 [==============================] - 28s 26ms/step - loss: 0.4715 - accuracy: 0.9284\n",
            "Epoch 625/650\n",
            "1069/1069 [==============================] - 29s 27ms/step - loss: 0.4699 - accuracy: 0.9267\n",
            "Epoch 626/650\n",
            "1069/1069 [==============================] - 27s 25ms/step - loss: 0.4663 - accuracy: 0.9306\n",
            "Epoch 627/650\n",
            "1069/1069 [==============================] - 27s 26ms/step - loss: 0.4630 - accuracy: 0.9292\n",
            "Epoch 628/650\n",
            "1069/1069 [==============================] - 28s 26ms/step - loss: 0.4698 - accuracy: 0.9268\n",
            "Epoch 629/650\n",
            "1069/1069 [==============================] - 29s 27ms/step - loss: 0.4663 - accuracy: 0.9284\n",
            "Epoch 630/650\n",
            "1069/1069 [==============================] - 28s 26ms/step - loss: 0.4599 - accuracy: 0.9294\n",
            "Epoch 631/650\n",
            "1069/1069 [==============================] - 30s 28ms/step - loss: 0.4647 - accuracy: 0.9280\n",
            "Epoch 632/650\n",
            "1069/1069 [==============================] - 29s 27ms/step - loss: 0.4583 - accuracy: 0.9306\n",
            "Epoch 633/650\n",
            "1069/1069 [==============================] - 28s 26ms/step - loss: 0.4590 - accuracy: 0.9296\n",
            "Epoch 634/650\n",
            "1069/1069 [==============================] - 28s 26ms/step - loss: 0.4554 - accuracy: 0.9316\n",
            "Epoch 635/650\n",
            "1069/1069 [==============================] - 28s 26ms/step - loss: 0.4578 - accuracy: 0.9298\n",
            "Epoch 636/650\n",
            "1069/1069 [==============================] - 28s 26ms/step - loss: 0.4536 - accuracy: 0.9304\n",
            "Epoch 637/650\n",
            "1069/1069 [==============================] - 28s 26ms/step - loss: 0.4555 - accuracy: 0.9302\n",
            "Epoch 638/650\n",
            "1069/1069 [==============================] - 29s 28ms/step - loss: 0.4555 - accuracy: 0.9305\n",
            "Epoch 639/650\n",
            "1069/1069 [==============================] - 28s 27ms/step - loss: 0.4486 - accuracy: 0.9321\n",
            "Epoch 640/650\n",
            "1069/1069 [==============================] - 30s 28ms/step - loss: 0.4505 - accuracy: 0.9317\n",
            "Epoch 641/650\n",
            "1069/1069 [==============================] - 29s 27ms/step - loss: 0.4479 - accuracy: 0.9322\n",
            "Epoch 642/650\n",
            "1069/1069 [==============================] - 30s 28ms/step - loss: 0.4440 - accuracy: 0.9338\n",
            "Epoch 643/650\n",
            "1069/1069 [==============================] - 29s 27ms/step - loss: 0.4469 - accuracy: 0.9308\n",
            "Epoch 644/650\n",
            "1069/1069 [==============================] - 29s 27ms/step - loss: 0.4463 - accuracy: 0.9305\n",
            "Epoch 645/650\n",
            "1069/1069 [==============================] - 32s 30ms/step - loss: 0.4417 - accuracy: 0.9332\n",
            "Epoch 646/650\n",
            "1069/1069 [==============================] - 28s 26ms/step - loss: 0.4465 - accuracy: 0.9323\n",
            "Epoch 647/650\n",
            "1069/1069 [==============================] - 29s 27ms/step - loss: 0.4424 - accuracy: 0.9322\n",
            "Epoch 648/650\n",
            "1069/1069 [==============================] - 28s 26ms/step - loss: 0.4373 - accuracy: 0.9342\n",
            "Epoch 649/650\n",
            "1069/1069 [==============================] - 28s 26ms/step - loss: 0.4378 - accuracy: 0.9340\n",
            "Epoch 650/650\n",
            "1069/1069 [==============================] - 29s 27ms/step - loss: 0.4386 - accuracy: 0.9340\n",
            "initial text: of the speckled band ix\n",
            "real text:['the', 'adventure', 'of', 'the', 'thumb', 'x', 'the', 'adventure', 'of', 'the', 'noble', 'bachelor', 'xi', 'the', 'adventure']\n",
            "recon_text:['the', 'adventure', 'of', 'the', 'thumb', 'x', 'but', 'adventure', 'of', 'the', 'noble', 'bachelor', 'xi', 'the', 'adventure']\n",
            "Total accuracy:0.964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using the model to reconstruct a text**\n",
        "* ## Measure the performance as accuracy in predicting the next word"
      ],
      "metadata": {
        "id": "4n-cPD4kHhlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # transorm the input \"text\" in one samples formed by its words.\n",
        "#   \"text\" should contain NPREV_WORDS words\n",
        "def prepare_input(text, NPREV_WORDS, tot_uniq_words, word_dict):\n",
        "    x = np.zeros((1, NPREV_WORDS, tot_uniq_words))\n",
        "    for t, word in enumerate(text.split()):\n",
        "        word = word.lower()\n",
        "        if word not in word_dict:\n",
        "          print(f\"word:{word} not in dictionary!\")\n",
        "          exit()\n",
        "        # stop when number of time steps reached\n",
        "        if t >= NPREV_WORDS:\n",
        "          break\n",
        "        else:\n",
        "          x[0, t, word_dict[word]] = 1\n",
        "    return x\n",
        "\n",
        "# input text to be predicted, the discitonry of words on which the model has\n",
        "# been trained\n",
        "def predict_completion(model, text, NPREV_WORDS, tot_uniq_words, word_dict):\n",
        "    original_text = text\n",
        "    generated = text\n",
        "    next_word = ''\n",
        "    x = prepare_input(text, NPREV_WORDS, tot_uniq_words, word_dict)\n",
        "    preds = model.predict(x, verbose=0)[0]\n",
        "    #print(f\"preds.shape:{preds.shape}\")\n",
        "    # taking next word as the one with maximum probability\n",
        "    #  should be extracted\n",
        "    next_index = np.argmax(preds)\n",
        "    next_word = list(word_dict.keys())[list(word_dict.values()).index(next_index)]\n",
        "    return next_word\n",
        "\n",
        "# number of words to recontstuct\n",
        "recon_len = 500\n",
        "ini = 190 # postion in the input text Where a whole sentence starts.\n",
        "\n",
        "test = words[ini:ini+NPREV_WORDS]\n",
        "# concatenate string list into a text separated\n",
        "test_text = \" \".join(test)\n",
        "print(f\"initial text: {test_text}\")\n",
        "recon_text = []\n",
        "# recontructing text with the predicted words in sequence, then compare with the true text\n",
        "for i in range(recon_len):\n",
        "  recon_text.append(predict_completion(model, test_text, NPREV_WORDS, tot_uniq_words, unique_word_index))\n",
        "  test[:NPREV_WORDS-1] = test[1:NPREV_WORDS]\n",
        "  test[NPREV_WORDS-1] = words[ini+i+NPREV_WORDS]\n",
        "  test_text = \" \".join(test)\n",
        "\n",
        "real_text = words[ini+NPREV_WORDS:ini+NPREV_WORDS+recon_len]\n",
        "accuracy = np.sum([1 if recon_text[i]== real_text[i] else 0 for i in range(recon_len)])/recon_len\n",
        "## Visual check\n",
        "print(f\"real text:{words[ini+NPREV_WORDS:ini+15+NPREV_WORDS]}\")\n",
        "print(f\"recon_text:{recon_text[:15]}\")\n",
        "print(f\"Total accuracy:{accuracy}\")"
      ],
      "metadata": {
        "id": "kB6wGav9Hfy2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}